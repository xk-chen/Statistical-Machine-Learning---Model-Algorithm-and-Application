{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build A Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudo code:\n",
    "\n",
    "    Function BuildTree(Data, Attribution):\n",
    "\n",
    "        if Attribution is empty or all Data labels are the same:\n",
    "\n",
    "            set node status as leaf\n",
    " \n",
    "            set class in this situation as most common class\n",
    "\n",
    "        else:\n",
    "\n",
    "            set node status as internal\n",
    "\n",
    "            use function bestAttribute(Data, Attribution) to gain a, the decision attribute\n",
    "\n",
    "            LeftNode = BuildTree(Data(satisfy a = 1), A exclude a)\n",
    "\n",
    "            RightNode = BuildTree(Data(satisfy a = 0), A exclude a)\n",
    "\n",
    "        end\n",
    "\n",
    "    end     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entropy** quantifies the amount the uncertainty associated with a specific probability distribution, it can be shown that the higher the entropy, the less confident we are in the outcome.\n",
    "    \n",
    "$$H(X) = \\sum_c -\\mathbb{P}(X = c)\\log_2\\mathbb{P}(X = c)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conditional Entropy** quantifies the amount the uncertainty given some information.\n",
    "    \n",
    "$$H(Y\\mid X) = \\sum_i \\mathbb{P}(X = i)H(Y\\mid X = i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Information Gain** is how much do we gain from knowing one of the attributes, in other words, the reduction in entropy.\n",
    "    \n",
    "$$IG(Y\\mid X) = H(Y) - H(Y\\mid X)\\geq 0$$\n",
    "    \n",
    "    \n",
    "\n",
    "\\begin{align*}\n",
    "IG(Y\\mid X) &= H(Y) - H(Y\\mid X)\\\\\n",
    "                &= \\sum_c -\\mathbb{P}(Y = c)\\log_2\\mathbb{P}(Y = c) - \\sum_i \\mathbb{P}(X = i)H(Y\\mid X = i)\\\\\n",
    "                &= \\sum_c -\\mathbb{P}(Y = c)\\log_2\\mathbb{P}(Y = c) - \\sum_i \\mathbb{P}(X = i)\\sum_c -\\mathbb{P}(Y = c\\mid X = i)\\log_2\\mathbb{P}(Y = c\\mid X = i)\\\\\n",
    "                &= -\\mathbb{E}[\\log_2p(y)] + \\mathbb{E}[\\mathbb{E}_X[\\log_2p(y\\mid X=i)]]\\\\\n",
    "                &\\geq -\\mathbb{E}[\\log_2p(y)] + \\log_2(\\mathbb{E}[\\mathbb{E}_X[p(y\\mid X=i)]])\\\\\n",
    "                &= -\\log_2p(y) + \\log_2p(y)\\\\\n",
    "                &= 0\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tree = {}\n",
    "        self.order = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        num_features = X.shape[-1]\n",
    "        X = np.vstack((np.arange(num_features), X))\n",
    "        self.tree = self.build_tree(X, y)\n",
    "        \n",
    "\n",
    "    def predict(self, X):\n",
    "        num_observation = X.shape[0]\n",
    "        prediction = np.zeros(num_observation)\n",
    "        for i in range(num_observation):\n",
    "            root = self.tree.keys()[0]\n",
    "            for j in range(X.shape[-1]):\n",
    "                self.tree.get(j) \n",
    "            \n",
    "    def build_tree(self, X, y):\n",
    "        if X.shape[-1] == 1:\n",
    "            count = self.class_count(y)\n",
    "            return max(count, key = count.get)\n",
    "        if len(np.unique(y)) == 1:\n",
    "            return y[0]\n",
    "        \n",
    "        optimal_feature = self.feature_selection(X, y)\n",
    "        self.order.append(optimal_feature)\n",
    "        optimal_feature_index = np.where(X[0] == optimal_feature)[0][0]\n",
    "        #print optimal_feature\n",
    "        tree = {optimal_feature: {}}\n",
    "        for val in np.unique(X[1:, optimal_feature_index]):\n",
    "            index = np.where(X[1:, optimal_feature_index] == val)[0]\n",
    "            X_tilde = np.vstack((X[0], X[index + 1]))\n",
    "            X_tilde = np.hstack((X_tilde[:, :optimal_feature_index], X_tilde[:, optimal_feature_index + 1: ]))\n",
    "            tree[optimal_feature][val] = self.build_tree(X_tilde, y[index])\n",
    "        return tree\n",
    "        \n",
    "    def feature_selection(self, X, y):\n",
    "        num_features = X.shape[-1]\n",
    "        information_gain_collection = np.zeros(num_features)\n",
    "        # Scan all features\n",
    "        for j in range(num_features):\n",
    "            conditional_entropy_collection = []\n",
    "            margin_probability_collection = []\n",
    "            # Scall all values of feature j\n",
    "            for val in np.unique(X[1:, j]):\n",
    "                # \\mathbb{H}[Y \\mid X(j)=val] = -\\sum_{y}\\mathbb{P}[Y=y \\mid X(j) = val] * log(\\mathbb{P}[Y=y \\mid X(j) = val])\n",
    "                index = np.where(X[1:, j] == val)[0]\n",
    "                distribution = self.class_distribution(y[index])\n",
    "                conditional_entropy = self.shannon_entropy(distribution)\n",
    "                conditional_entropy_collection.append(conditional_entropy)\n",
    "                # \\mathbb{P}[X(j) = val] = \\frac{\\sum_i \\mathbb{1}[X_i(j) = val]}{n}\n",
    "                margin_probability_collection.append(len(index) * 1.0 / len(X))\n",
    "            # \\mathbb{H}[Y \\mid X(j)] = \\sum_{val} \\mathbb{P}[X(j) = val]\\mathbb{H}[Y \\mid X(j) = val]\n",
    "            information_gain_collection[j] = self.shannon_entropy(self.class_distribution(y)) - \\\n",
    "                                             np.dot(np.array(margin_probability_collection), \n",
    "                                                    np.array(conditional_entropy_collection))\n",
    "        return X[0, np.argmax(information_gain_collection)]\n",
    "\n",
    "    def shannon_entropy(self, distribution):\n",
    "        return -np.dot(distribution, np.log2(distribution))\n",
    "    \n",
    "    def class_count(self, labels):\n",
    "        label_count = {}\n",
    "        for label in labels:\n",
    "            if label not in label_count.keys(): label_count[label] = 1\n",
    "            else: label_count[label] += 1\n",
    "        return label_count\n",
    "    \n",
    "    def class_distribution(self, labels):\n",
    "        label_count = self.class_count(labels)\n",
    "        v = np.array(label_count.values())\n",
    "        return v * 1.0 / v.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.array([[\"Comedy\",\"Short\",\"Adamson\",\"No\",\"Yes\"],[\"Animated\",\"Short\",\"Lasseter\",\"No\",\"No\"],\n",
    "                          [\"Drama\",\"Medium\",\"Adamson\",\"No\",\"Yes\"],[\"Animated\",\"Long\",\"Lasseter\",\"Yes\",\"No\"],\n",
    "                          [\"Comedy\",\"Long\",\"Lasseter\",\"Yes\",\"No\"],[\"Drama\",\"Medium\",\"Singer\",\"Yes\",\"Yes\"],\n",
    "                          [\"Animated\",\"Short\",\"Singer\",\"No\",\"Yes\"],[\"Comedy\",\"Long\",\"Adamson\",\"Yes\",\"Yes\"],\n",
    "                          [\"Drama\",\"Medium\",\"Lasseter\",\"No\",\"Yes\"]])\n",
    "X = dataset[:,:-1]\n",
    "y = dataset[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2': {'Adamson': 'Yes',\n",
       "  'Lasseter': {'0': {'Animated': 'No', 'Comedy': 'No', 'Drama': 'Yes'}},\n",
       "  'Singer': 'Yes'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.tree"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
