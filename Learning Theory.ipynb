{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 0,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "## Statistical Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 7,
        "hidden": false,
        "row": 0,
        "width": null
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "Given sample $X^n\\sim \\mathcal{D}$, where $X^n=\\{X_1,X_2,\\dots,X_n\\}$, we want to know some information $\\theta=g(\\mathcal{D})$, where $\\theta\\in\\Theta$.\n",
    "\n",
    "TODO:\n",
    "\n",
    "Use sample $X^n$, with the help of some method such as MLE, to estimate $\\theta$: $\\hat{\\theta}_{MLE}=T(X^n)$, we call $T(\\cdot)$ is statistical functional. And $\\hat{\\theta}_{MLE}$ has a large amount of good properties. $(\\hat{\\theta}_{MLE}\\stackrel{P}{\\rightarrow}\\theta, \\hat{\\theta}_{MLE} - \\theta \\sim \\mathcal{N}(0,\\sigma^2(n)),\\cdots$).\n",
    "\n",
    "Heuristics: Use some method $\\mathcal{M}$ and sample(or data) $\\mathcal{S}$ to search parameter space $\\Theta$ for optimal model $\\theta^*$.\n",
    "\n",
    "Then we can do some analog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 4,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 90,
        "hidden": false,
        "row": 7,
        "width": null
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "Q: Learn what?\n",
    "\n",
    "A: Some concept $c\\in\\mathcal{C}$.\n",
    "\n",
    "Procedure: Algorithm $\\mathcal{A}$ takes data $\\mathcal{D}$ to find a hypothesis $f$ from hypothesis space $\\mathcal{F}$ for estimating the true concept $c$.\n",
    "\n",
    "Learning Theory tells us\n",
    "\n",
    "* How good are these algorithms on unknown test sets?\n",
    "\n",
    "* How many training samples do we need to achieve small error?\n",
    "\n",
    "* What is the smallest error we can achieve?\n",
    "\n",
    "### Notation\n",
    "\n",
    "Loss function:\n",
    "\n",
    "$$L(x,y,f(x))$$\n",
    "\n",
    "where $L:\\mathcal{X}\\times\\mathcal{Y}\\times \\mathbb{R}\\to [0,\\infty)$.\n",
    "\n",
    "Risk:\n",
    "\n",
    "$$R(f)\\equiv R_{L,P}(f)=\\mathbb{E}_{(x,y)\\sim P}[L(x,y,f(x))]=\\int_{\\mathcal{X}\\times\\mathcal{Y}}L(x,y,f(x))dP(x,y)$$\n",
    "\n",
    "Bayes Risk:\n",
    "\n",
    "$$R^*\\equiv R_{L,P}^*=\\inf_{f:\\mathcal{X}\\to\\mathcal{Y}}\\int_{\\mathcal{X}\\times\\mathcal{Y}}L(x,y,f(x))dP(x,y)$$\n",
    "\n",
    "Empirical Risk:\n",
    "\n",
    "$$\\hat{R}_n(f)=\\frac{1}{n}\\sum_{i=1}^n L(Y_i,f(X_i))$$\n",
    "\n",
    "### Big Picture\n",
    "\n",
    "<div style = \"text-align:center\">\n",
    "<img src=\"Pictures\\VC.png\", title = \"Big Picture\", width = \"600\", height = \"300\"/>\n",
    "<caption><B>FIGURE 1: The big picture of learning</B></caption>\n",
    "</div>\n",
    "\n",
    "**Goal of learning: Building a function $f_{n,\\mathcal{F}}^*$ whose risk $R(f_{n,\\mathcal{F}}^*)$ close to Bayes Risk**.\n",
    "\n",
    "TODO: \n",
    "\n",
    "$$\\big|\\hat{R}_n(f_{n,\\mathcal{F}}^*)-R^*\\big|\\leq \\big|\\hat{R}_n(f_{n,\\mathcal{F}}^*)-R^*_{\\mathcal{F}}\\big|+\\big|R^*_{\\mathcal{F}}-R^*\\big |\\leq \\underbrace{\\big|\\hat{R}_n(f_{n,\\mathcal{F}}^*)-R(f_{n,\\mathcal{F}}^*)\\big|}_{\\text{sketch: LLN} }+\\underbrace{\\big|R(f_{n,\\mathcal{F}}^*)-R^*_{\\mathcal{F}}\\big|}_{\\text{estimation error} }+\\underbrace{\\big|R^*_{\\mathcal{F}}-R^*\\big |}_{\\text{approximation error}}$$\n",
    "\n",
    "\\begin{theorem}[Law of Large Numbers]\n",
    "\n",
    "Given iid random variables $z^L$, when $L\\rightarrow\\infty$, sample mean will converge to the population mean.\n",
    "\n",
    "$$\\mathbb{P}\\Big(\\big|{\\frac{1}{L}\\sum_{l=1}^Lf(z_l)-\\mathbb{E}[f(z)]}\\big|>\\epsilon\\Big)\\rightarrow 0$$\n",
    "\n",
    "\\end{theorem}\n",
    "\n",
    "Learning Frameworks:\n",
    "\n",
    "### Probably Approximately Correct (PAC) Theory\n",
    "\n",
    "In this scenario, we require optimal hypothesis $f$ close to the concept $c$ with arbitrary accuracy and with high probability.\n",
    "\n",
    "i.e. $\\forall \\epsilon,\\delta$, we have\n",
    "\n",
    "$$\\mathbb{P}(R(f)>\\epsilon)<\\delta$$\n",
    "\n",
    "where $R(f)$ is the risk on the population, $R(f)=\\mathbb{P}(f(x)\\not=c(x))$.\n",
    "\n",
    "In other words, it reveals that $c$ must lie in $\\mathcal{F}$.\n",
    "\n",
    "To find $f$ that satisfies PAC condition, it is nature to think about $f$ that performs perfectly on training set $\\mathcal{S}$, we\n",
    "define version space $\\mathcal{F'}=\\{f\\in\\mathcal{F}:\\hat{R}_n(f)=0\\}$. And any $f\\in\\mathcal{F'}$ called consistent learner.\n",
    "\n",
    "\\begin{theorem}[Haussler 1988]\n",
    "\n",
    "Assume $\\big|\\mathcal{F'}\\big|<\\infty$, $\\mathbb{P}(\\exists f \\in \\mathcal{F'}: \\hat{R}_n(f)=0 \\wedge R(f)>\\epsilon)\\leq \\big|\\mathcal{F'}\\big|e^{-\\epsilon m}$, where $m$ is the length of sample.\n",
    "\n",
    "\\end{theorem}\n",
    "\n",
    "\\begin{proof}\n",
    "\n",
    "\n",
    "$$\\mathbb{P}(\\exists f \\in \\mathcal{F'}: \\hat{R}_n(f)=0 \\wedge R(f)>\\epsilon)\\leq \\sum_{f\\in \\mathcal{F'}}\\mathbb{P}(\\hat{R}_n(f)=0 \\wedge R(f)>\\epsilon)\n",
    "\\leq \\big|\\mathcal{F'}\\big|(1-\\epsilon)^m\n",
    "\\leq \\big|\\mathcal{F'}\\big|e^{-\\epsilon m}$$\n",
    "\n",
    "\\end{proof}\n",
    "\n",
    "If we choose $f\\in\\mathcal{F'}$, we can bound the error on $\\mathcal{D}$.\n",
    "\n",
    "Let $|\\mathcal{F'}|e^{-\\epsilon m}\\leq\\delta$, we have sample complexity $m>\\frac{1}{\\epsilon}\\log{\\frac{|\\mathcal{F'}|}{\\delta}}$.\n",
    "\n",
    "### Agnostic Learning\n",
    "\n",
    "Here we do not assume $c\\in\\mathcal{F}$, we want choose $f\\in\\mathcal{F}$ so that minimize $R(f)$.\n",
    "\n",
    "To achieve the goal of learning, we need to bound three terms.\n",
    "\n",
    "Minimizing the approximation error $\\big|R^*_{\\mathcal{F}}-R^*\\big |$ is to choose a model set $\\mathcal{F}$ as large as possible.\n",
    "\n",
    "And the term\n",
    "\n",
    "$$\\big|\\hat{R}_n(f_{n,\\mathcal{F}}^*)-R(f_{n,\\mathcal{F}}^*)\\big|\\leq\\sup_{f\\in\\mathcal{F}}\\big|\\hat{R}_n(f)-R(f)\\big|$$\n",
    "\n",
    "Meanwhile, the estimation error\n",
    "\n",
    "$$\\big|R(f_{n,\\mathcal{F}}^*)-R^*_{\\mathcal{F}}\\big|=R(f_{n,\\mathcal{F}}^*)-R^*_{\\mathcal{F}}=\\underbrace{R(f_{n,\\mathcal{F}}^*)-\\hat{R}_n(f_{n,\\mathcal{F}}^*)}_{\\leq \\sup_{f\\in\\mathcal{F}}\\big|\\hat{R}_n(f)-R(f)\\big|}+\\underbrace{\\hat{R}_n(f_{n,\\mathcal{F}}^*)-\\hat{R}^*_{\\mathcal{F}}}_{<0}+\\underbrace{\\hat{R}^*_{\\mathcal{F}}-R^*_{\\mathcal{F}}}_{\\leq \\sup_{f\\in\\mathcal{F}}\\big|\\hat{R}_n(f)-R(f)\\big|}$$\n",
    "\n",
    "Hence, the estimation error can be bounded by\n",
    "\n",
    "$$\\big|\\hat{R}_n(f_{n,\\mathcal{F}}^*)-R(f_{n,\\mathcal{F}}^*)\\big| + \\big|R(f_{n,\\mathcal{F}}^*)-R^*_{\\mathcal{F}}\\big| \\leq 3\\sup_{f\\in\\mathcal{F}}\\big|\\hat{R}_n(f)-R(f)\\big|$$\n",
    "\n",
    "Heuristics:\n",
    "\n",
    "If we can show \n",
    "\n",
    "$$\\mathbb{P}\\big(\\sup_{f\\in\\mathcal{F}}\\big|\\hat{R}_n(f)-R(f)\\big|>\\epsilon \\big)\\leq \\text{ something small}$$\n",
    "\n",
    "Minimize $\\hat{R}_n(f)$ will bound $R(f)$, learning is possible $\\color{red}{:)}$\n",
    "\n",
    "We call this Empirical Risk Minimization.\n",
    "\n",
    "\\begin{theorem}[Hoeffding 1963]\n",
    "\n",
    "If $X$ is a random variable satisfies $X\\sim \\text{Bernoulli}(p)$, then \n",
    "\n",
    "$$\\mathbb{P}\\big(\\big|\\hat{p}-p\\big|>\\epsilon \\big)\\leq 2e^{-2m\\epsilon^2}$$\n",
    "\n",
    "where $\\hat{p}=\\bar{X}=\\frac{1}{m}\\sum_{i}X_i$\n",
    "\n",
    "\\end{theorem}\n",
    "\n",
    "Set $p=R(f)$, $\\hat{p}=\\hat{R}_n(f)$, we have:\n",
    "\n",
    "$$\\mathbb{P}\\big(\\big|\\hat{R}_n(f)-R(f)\\big|>\\epsilon \\big)\\leq 2e^{-2m\\epsilon^2}$$\n",
    "\n",
    "\n",
    "\\begin{theorem}\n",
    "If $k = \\big|\\mathcal{F}\\big|<\\infty$, for all $m,\\delta$, with probability $1-\\delta$:\n",
    "\n",
    "$$\\sup_{f\\in\\mathcal{F}}\\big|\\hat{R}_n(f)-R(f)\\big|<\\epsilon$$\n",
    "\n",
    "\\end{theorem}\n",
    "\n",
    "\\begin{proof}\n",
    "\n",
    "According to Hoeffding Inequality, \n",
    "\n",
    "$$\\mathbb{P}\\big(\\big|\\hat{R}_n(f)-R(f)\\big|>\\epsilon \\big)\\leq 2e^{-2m\\epsilon^2}$$\n",
    "\n",
    "$$\\mathbb{P}\\big(\\sup_{f\\in\\mathcal{F}}\\big|\\hat{R}_n(f)-R(f)\\big|>\\epsilon \\big)\\leq \\sum_{f\\in \\mathcal{F}}\\mathbb{P}\\big(\\big|\\hat{R}_n(f)-R(f)\\big|>\\epsilon \\big)\\leq 2ke^{-2m\\epsilon^2}=\\delta$$\n",
    "\n",
    "\\end{proof}\n",
    "\n",
    "And to obtain the bound $\\delta$, the sample complexity is $m\\geq \\frac{1}{2\\epsilon^2}\\log{\\frac{2k}{\\delta}}$. Therefore, we bound the ERM hypothesis on the population.\n",
    "\n",
    "### VC Theory and Structual Risk Minimization\n",
    "\n",
    "Q: How about $\\big|\\mathcal{F}\\big|$ is not finite?\n",
    "\n",
    "A: We need some other measure of complexity for $\\mathcal{F}$ - VC dimension.\n",
    "\n",
    "\\begin{definition}[Shattering]\n",
    "Let $F=\\{x_1,\\dots,x_n\\}$ be a finite set, $G$ be a subset of $F$. If $A\\cap F = G$ for some $A\\in \\mathcal{A}$. We call $A$ picks out $G$. $S(A,F)\\leq 2^n$ is the number of subsets picked out by $A$. If $S(A,F)=2^n$, we call $A$ shatters $F$.\n",
    "\\end{definition}\n",
    "\n",
    "\\begin{definition}[Shatter coefficient]\n",
    "\n",
    "$$S_n(A)=\\sup_{F\\in\\mathcal{F}_n}S(A,F)$$\n",
    "\n",
    "Note: $\\mathcal{F}_n$ is a class of sets that has $n$ element, we want to find the set that can be picked out most points by $A$.\n",
    "\n",
    "\\end{definition}\n",
    "\n",
    "\n",
    "\n",
    "\\begin{theorem}[Vapnik and Chervonenkis 1971]\n",
    "\n",
    "$$\\mathbb{P}\\big(\\sup_{A\\in\\mathcal{A}}\\big|P_n(A)-P(A)\\big|>\\epsilon \\big)\\leq 8S_n(\\mathcal{A})e^{-\\frac{n\\epsilon^2}{32}}$$\n",
    "\n",
    "\\end{theorem}\n",
    "\n",
    "\n",
    "\\begin{definition}[VC dimension]\n",
    "\n",
    "$$d=\\text{largest } n \\text{ such that } S_n(\\mathcal{A})=2^n  $$\n",
    "\n",
    "It means that VC dimension is the maximum $n$ that $F$ can be shattered.\n",
    "\n",
    "\\end{definition}\n",
    "\n",
    "\\begin{theorem}[Sauer 1972]\n",
    "\n",
    "If VC dimension $d<\\infty$, $\\forall n\\geq d$, $S_n(\\mathcal{A})\\leq (n+1)^d$.\n",
    "\n",
    "\\end{theorem}\n",
    "\n",
    "Therefore, if $n\\geq d$\n",
    "\n",
    "$$\\mathbb{P}\\big(\\sup_{A\\in\\mathcal{A}}\\big|P_n(A)-P(A)\\big|>\\epsilon \\big)\\leq 8(n+1)^de^{-\\frac{n\\epsilon^2}{32}}$$\n",
    "\n",
    "Return to our scenario,\n",
    "\n",
    "$$\\mathbb{P}\\big(\\sup_{f\\in\\mathcal{F}}\\big|\\hat{R}_n(f)-R(f)\\big|>\\epsilon \\big)\\leq 8(n+1)^de^{-\\frac{n\\epsilon^2}{32}}<\\delta$$\n",
    "\n",
    "\n",
    "\n",
    "It means that $\\forall f\\in \\mathcal{F},\\hat{R}_n(f)\\stackrel{P}{\\rightarrow}R(f)$.\n",
    "\n",
    "With probability $1-\\delta$\n",
    "\n",
    "$$R^*_{\\mathcal{F}}\\in \\hat{R}_n(f_{n,\\mathcal{F}}^*)\\pm 3\\epsilon(\\delta,m,d)$$\n",
    "\n",
    "In other words, \n",
    "\n",
    "$$\\text{Risk Expectation} = \\text{Empirical Risk} + \\text{CI}   $$\n",
    "\n",
    "\n",
    "\n",
    "Another idea: minimize the bound on true error $R(f)$, i.e. $\\hat{R}_n(f) + \\epsilon$\n",
    "\n",
    "It is called Structural Risk Minimization: find model subset $i$ that expected risk $R(f)$ becomes minimum, for a optimal VC dimension $d^*=d_i$, relating to a specific family $\\mathcal{F}_i$ of the model sequence, then build model using $f\\in \\mathcal{F}_i$.\n",
    "\n",
    "So far, we achieve with high probability, $\\big|R(f_{n,\\mathcal{F}}^*)-R^*_{\\mathcal{F}}\\big|\\rightarrow 0$, it is not enough, to obtain the ultimate goal $\\big|R(f_{n,\\mathcal{F}}^*)-R^*\\big|\\rightarrow 0$, we hope the approximation error\n",
    "\n",
    "$$\\big|R^*_{\\mathcal{F}}-R^*\\big |\\rightarrow 0$$\n",
    "\n",
    "How? Let $\\mathcal{F}_1\\subset \\mathcal{F}_2\\subset\\dots\\subset\\mathcal{F}_n\\subset\\dots $ that $VC_{\\mathcal{F}_1}\\leq VC_{\\mathcal{F}_2}\\leq\\dots VC_{\\mathcal{F}_n}\\leq\\dots$ We need the larger set. But it violates the SRM...\n",
    " \n",
    "\n",
    "Bias-Variance trade-off:\n",
    "\n",
    "If we want to minimize $\\hat{R}_n(f)$, we need to search in larger space $\\mathcal{F}$, causing $d\\uparrow$, so as $\\epsilon$.\n",
    "\n",
    "If we want to minimize $\\epsilon(\\delta,m,d)$, we should let $d\\downarrow$, then $\\hat{R}_n(f)\\uparrow$."
   ]
  }
 ],
 "metadata": {
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "report_default",
    "version": 1,
    "views": {
     "grid_default": {
      "cellMargin": 10,
      "defaultCellHeight": 20,
      "maxColumns": 12,
      "name": "grid",
      "type": "grid"
     },
     "report_default": {
      "name": "report",
      "type": "report"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "117px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
