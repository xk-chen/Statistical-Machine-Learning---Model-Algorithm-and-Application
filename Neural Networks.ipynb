{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have modeled perceptron learning, while perceptron can not solve the non-separable problems, one of the methods is feature mapping, the other is the multi-layer perceptron(i.e. full-connection neural network). MLP adds hidden neurons to the model.\n",
    "\n",
    "Moreover, in perceptron learning algorithm, the function $\\text{sign}(\\cdot)$ is hard to optimize. Here we need a differentiable objective which can be solved in gradient-based method. The most popular **activation function** of the \"**soma**\" is Sigmoid function\n",
    "\n",
    "$$\\sigma(x)=\\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "The nice property of $\\sigma(\\cdot)$ is\n",
    "\n",
    "$$\\frac{\\partial\\sigma(x)}{\\partial x}=\\sigma(x)(1-\\sigma(x))$$\n",
    "\n",
    "<div style = \"text-align:center\">\n",
    "<img src=\"Pictures\\MLP.png\", title = \"Multi-Layer Perceptron\", width = \"400\", height = \"300\"/>\n",
    "<caption><B>FIGURE 1: The Architecture of Multi-Layer Perceptron</B></caption>\n",
    "</div>\n",
    "\n",
    "Consider $n_l$ layers network, assume the number of nodes in $l$-th layer is $s_l$.\n",
    "\n",
    "Define the Loss function\n",
    "\n",
    "$$J(W,b)=\\big[\\frac{1}{m}\\sum_{i=1}^mJ(W,b;x^{(i)},y^{(i)})\\big]+\\frac{\\lambda}{2}\\sum_{l=1}^{n_l}\\sum_i^{s_l}\\sum_j^{s_{l+1}}(W_{ij}^{(l)})^2$$\n",
    "\n",
    "Here we adapt the error measure $MSE$,\n",
    "\n",
    "$$J(W,b)=\\Big[\\frac{1}{m}\\sum_{i=1}^m\\Big(\\frac{1}{2}\\big\\|h_{W,b}(x^{(i)})-y^{(i)}\\big\\|^2\\Big)\\Big]+\\frac{\\lambda}{2}\\sum_{l=1}^{n_l}\\sum_i^{s_l}\\sum_j^{s_{l+1}}(W_{ij}^{(l)})^2$$\n",
    "\n",
    "Recall the gradient descent method, the update rule:\n",
    "\n",
    "$$W_{ij}^{(l)}=W_{ij}^{(l)}-\\alpha\\frac{\\partial J(W,b)}{\\partial W_{ij}^{(l)}}$$\n",
    "\n",
    "$$b_i^{(l)}=b_i^{(l)}-\\alpha\\frac{\\partial J(W,b)}{\\partial b_i^{(l)}}$$\n",
    "\n",
    "where $\\alpha$ is the learning rate.\n",
    "\n",
    "Then we compute the gradient\n",
    "\n",
    "$$\\frac{\\partial J(W,b)}{\\partial W_{ij}}=\\Big[\\frac{1}{m}\\sum_{i=1}^m\\frac{\\partial J(W,b;x^{(i)},y^{(i)})}{\\partial W_{ij}}\\Big]+\\lambda W_{ij}^{(l)}$$\n",
    "\n",
    "$$\\frac{\\partial J(W,b)}{\\partial b_i^{(l)}}=\\frac{1}{m}\\sum_{i=1}^m\\frac{\\partial J(W,b;x^{(i)},y^{(i)})}{\\partial b_i^{(l)}}$$\n",
    "\n",
    "For each layer, we need **back-propagation** algorithm. The heuristic is for each sample $(x,y)$, we use forward-propagation to compute the inner product at layer $l$ and node $j$, called $z_j^{(l)}$ , and the output of layer $l$ and node $j$ - $a_j^{(l)}$ until output layer. Then compute the \"residual\" at $l$-th layer and $i$-th node $\\delta_i^{(l)}$ until the input layer.\n",
    "\n",
    "\n",
    "For the output layer $n_l$, the residual is\n",
    "\n",
    "\\begin{align*}\n",
    "\\delta_i^{(n_l)} &= \\frac{\\partial}{\\partial z_i^{(n_l)}}J(W,b;x,y)\\\\\n",
    "&= \\frac{\\partial}{\\partial z_i^{(n_l)}}\\frac{1}{2}\\big\\|y-h_{W,b}(x)\\big\\|^2\\\\\n",
    "&=\\frac{\\partial}{\\partial z_i^{(n_l)}}\\frac{1}{2}\\sum_{j=1}^{S_{n_l}}(y_j-a_j^{(n_l)})^2\\\\\n",
    "&=\\frac{\\partial}{\\partial z_i^{(n_l)}}\\frac{1}{2}\\sum_{j=1}^{S_{n_l}}(y_j-\\sigma(z_j^{(n_l)})^2\\\\\n",
    "&=-(y_i-\\sigma(z_i^{(n_l)}))\\cdot\\sigma'(z_i^{(n_l)})\\\\\n",
    "&=-(y_i-a_i^{(n_l)})\\cdot\\sigma'(z_i^{(n_l)})\\\\\n",
    "\\end{align*}\n",
    "\n",
    "For the layer $l=n_l-1,n_l-2,\\dots,2$, the residual of $i$-th node at $l$-th layer can be shown as\n",
    "\n",
    "\\begin{align*}\n",
    "\\delta_i^{(l)}&=\\frac{\\partial}{\\partial z_i^{(l)}}J(W,b;x,y)\\\\\n",
    "&=\\frac{\\partial}{\\partial z_i^{(l)}}\\frac{1}{2}\\big\\|y-h_{W,b}(x)\\big\\|^2\\\\\n",
    "&=\\frac{\\partial}{\\partial z_i^{(l)}}\\frac{1}{2}\\sum_{j=1}^{S_{l+1}}(y_j-a_j^{(n_{l+1})})^2\\\\\n",
    "&=\\frac{1}{2}\\sum_{j=1}^{S_{l+1}}\\frac{\\partial}{\\partial z_i^{(l)}}\\big(y_j-\\sigma(z_j^{(l+1)})\\big)^2\\\\\n",
    "&=\\sum_{j=1}^{S_{l+1}}-\\big(y_i-\\sigma(z_j^{(l+1)})\\big)\\cdot\\frac{\\partial}{\\partial z_i^{(l)}}\\sigma(z_j^{(l+1)})\\\\\n",
    "&=\\sum_{j=1}^{S_{l+1}}-\\big(y_i-\\sigma(z_j^{(l+1)})\\big)\\cdot\\sigma'(z_j^{(l+1)})\\cdot\\frac{\\partial z_i^{(l+1)}}{\\partial z_i^{(l)}}\\\\\n",
    "&=\\sum_{j=1}^{S_{l+1}}\\delta_j^{(l+1)}\\cdot\\frac{\\partial z_i^{(l+1)}}{\\partial z_i^{(l)}}\\\\\n",
    "&=\\sum_{j=1}^{S_{l+1}}\\Big(\\delta_j^{(l+1)}\\cdot\\frac{\\partial }{\\partial z_i^{(l)}}\\sum_{k=1}^{S_l}\\sigma(z_k^{(l)})\\cdot W_{jk}^{(l)}\\Big)\\\\\n",
    "&=\\sum_{j=1}^{S_{l+1}}\\delta_j^{(l+1)}\\cdot W_{ji}^{(l)}\\cdot \\sigma(z_i^{(l)})\\\\\n",
    "&=\\Big(\\sum_{j=1}^{S_{l+1}}W_{ji}^{(l)}\\delta_j^{(l+1)}\\Big)\\sigma(z_i^{(l)})\n",
    "\\end{align*}\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$\\frac{\\partial J(W,b;x,y)}{\\partial W_{ij}^{(l)}}=\\frac{\\partial J(W,b;x,y)}{\\partial z_i^{(l+1)}}\\frac{\\partial z_i^{(l+1)}}{\\partial W_{ij}^{(l)}}=\\delta_i^{(l+1)}a_j^{(l)}$$\n",
    "\n",
    "$$\\frac{\\partial J(W,b;x,y)}{\\partial b_{i}^{(l)}}=\\frac{\\partial J(W,b;x,y)}{\\partial z_i^{(l+1)}}\\frac{\\partial z_i^{(l+1)}}{\\partial b_{i}^{(l)}}=\\delta_i^{(l+1)}$$\n",
    "\n",
    "The regularization term of layer $l$ can be get easily\n",
    "\n",
    "$$R^{(l)}=\\lambda W^{(l)}$$\n",
    "\n",
    "Summary, an epoch of computation can be shown as follows:\n",
    "\n",
    "1.Conduct the forward-propagation algorithm, get the output $a^{(2)},a^{(3)},\\dots,a^{(n_l)}$.\n",
    "\n",
    "2.Compute the output layer error\n",
    "\n",
    "$$\\delta^{(n_l)}=-(y-a^{(n_l)})\\odot\\sigma'(z^{(n_l)})$$\n",
    "\n",
    "3.Then the others, for $l=n_l-1,\\dots,2$\n",
    "\n",
    "$$\\delta^{(l)}=(W^{(l)T}\\delta^{(l+1)})\\odot\\sigma'(z^{(l)})$$\n",
    "\n",
    "4.Compute the gradient of $(x^{(i)},y^{(i)})$\n",
    "\n",
    "$$\\nabla_{W^{(l)}}J(W,b;x^{(i)},y^{(i)})=\\delta^{(l+1)}(a^{(l)})^T$$\n",
    "\n",
    "$$\\nabla_{b^{(l)}}J(W,b;x^{(i)},y^{(i)})=\\delta^{(l+1)}$$\n",
    "\n",
    "5.Update parameters\n",
    "\n",
    "$$W^{(l)}=W^{(l)}-\\alpha\\Big[\\big(\\frac{1}{m}\\sum_i\\nabla_{W^{(l)}}J(W,b;x^{(i)},y^{(i)})\\big)+\\lambda W^{(l)}\\Big]$$\n",
    "\n",
    "$$b^{(l)}=b^{(l)}-\\alpha\\Big[\\frac{1}{m}\\sum_i\\nabla_{b^{(l)}}J(W,b;x^{(i)},y^{(i)})\\Big]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        assert self.name in [\"sigmoid\", \"tanh\", \"relu\", \"softmax\"], \"Illegal activation function.\"\n",
    "        \n",
    "    def evaluation(self, Z):\n",
    "        if self.name == \"sigmoid\":\n",
    "            return 1.0 / (1 + np.exp(-Z))\n",
    "        \n",
    "        elif self.name == \"tanh\":\n",
    "            return np.tanh(Z)\n",
    "        \n",
    "        elif self.name == \"softmax\":\n",
    "            Z = Z - np.max(Z, axis = 0, keepdims = True)\n",
    "            Z = np.exp(Z)\n",
    "            Z = Z / np.sum(Z, axis = 0, keepdims = True) \n",
    "            return Z\n",
    "        \n",
    "        else:\n",
    "            return np.maximum(0, Z)\n",
    "        \n",
    "    def derivative(self, A):\n",
    "        \n",
    "        if self.name == \"sigmoid\":\n",
    "            return A * (1 - A)\n",
    "        \n",
    "        elif self.name == \"tanh\":\n",
    "            return 1 - np.power(A, 2)\n",
    "        \n",
    "        elif self.name == \"softmax\":\n",
    "            return A\n",
    "        \n",
    "        else:\n",
    "            return (A > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        assert self.name in [\"logistic\", \"multiclass_logistic\"], \"Illegal loss function\"\n",
    "    \n",
    "    def evaluation(self, A, Y):\n",
    "        if self.name == \"logistic\":\n",
    "            return -1.0 / Y.shape[-1] * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))\n",
    "        \n",
    "        elif self.name == \"multiclass_logistic\":\n",
    "            return -1.0 / Y.shape[-1] * np.sum(Y * np.log(A))\n",
    "    \n",
    "    def derivative(self, A, Y):\n",
    "        if self.name == \"logistic\":\n",
    "            return -(Y / A - (1 - Y) / (1 - A))\n",
    "        elif self.name == \"multiclass_logistic\":\n",
    "            #return - Y / A\n",
    "            return Y - A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regularizer:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        assert self.name in [\"Frobenius\"], \"Illegal loss function\"\n",
    "    def evaluation(self, weights, num_layers, num_samples, regularization_coefficient):\n",
    "        summation = 0.0\n",
    "        for l in range(1, num_layers + 1):\n",
    "            summation += np.linalg.norm(weights[\"Theta\" + str(l)], ord = \"fro\") ** 2\n",
    "        return 1.0 * regularization_coefficient / (2 * num_samples) * summation\n",
    "    \n",
    "    def derivative(self, weights, layer, num_samples, regularization_coefficient):\n",
    "        return regularization_coefficient * weights[\"Theta\" + str(layer)] / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, num_layers, hidden_units, activations, loss = \"logistic\", dropout = [], keep_prob = 0.5, \n",
    "                 initialization = \"He\", std = 0.01, bias = 0, regularizer = \"Frobenius\", regularization_coefficient = 0.0):\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_units = hidden_units\n",
    "        self.activations = {}\n",
    "        self.loss_function = Loss(name = loss)\n",
    "        self.regularizer = Regularizer(name = regularizer)\n",
    "        self.regularization_coefficient = regularization_coefficient\n",
    "        self.loss_collection = []\n",
    "        self.dropout = dropout\n",
    "        self.keep_prob = keep_prob\n",
    "        self.initialization = initialization\n",
    "        self.std = std\n",
    "        self.bias = bias\n",
    "        \n",
    "        assert self.std > 0, \"The standard deviation of weights should be positive\"\n",
    "        assert self.num_layers == len(self.hidden_units), \"The dimension of hidden units should be the same with num_layers.\"\n",
    "        assert self.num_layers == len(activations), '''The dimension of activation functions should be \n",
    "                                                                the same with num_layers.'''\n",
    "        for l in range(1, self.num_layers + 1):\n",
    "            self.activations[\"activation\" + str(l)] = Activation(activations[l - 1])\n",
    "            \n",
    "        \n",
    "    def training(self, X, y, learning_rate = 0.01, mini_batch_size = 64, num_epochs = 1000, random_seed = 0, optimizer = \"Adam\", \n",
    "                 decay_for_avg_grad = 0.9, decay_for_squared_grad = 0.999, epsilon = 1e-8, decay_rate = 0.0):\n",
    "        \n",
    "        \n",
    "        self.hidden_units.insert(0, X.shape[-1])\n",
    "        self.learning_rate = learning_rate\n",
    "        self.parameters = {}\n",
    "        self.initialize(initialization = self.initialization, std = self.std, bias = self.bias, random_seed = random_seed)\n",
    "        self.optimizer = optimizer\n",
    "        self.caches = {}        \n",
    "        \n",
    "        mini_batches = self.mini_batch_split(X, y, mini_batch_size = mini_batch_size, random_seed = random_seed)\n",
    "        \n",
    "        for epoch in range(1, num_epochs + 1):\n",
    "            num_observations = None\n",
    "            for minibatch in mini_batches:\n",
    "                X, y = minibatch\n",
    "                num_observations = X.shape[-1]\n",
    "                self.forward_propagation(X, y)\n",
    "                self.backward_propagation(X, y)\n",
    "                self.optimize(optimizer = self.optimizer, learning_rate = self.learning_rate, epoch = epoch, \n",
    "                              decay_for_avg_grad = decay_for_avg_grad, decay_for_squared_grad = decay_for_squared_grad,\n",
    "                              epsilon = epsilon)\n",
    "            loss = self.loss_function.evaluation(self.caches[\"A\" + str(self.num_layers)], y) \\\n",
    "                   + self.regularizer.evaluation(self.parameters, self.num_layers, num_observations, \n",
    "                   self.regularization_coefficient)\n",
    "            self.loss_collection.append(loss)\n",
    "            self.learning_rate *= 1.0 / (1 + decay_rate * epoch)\n",
    "            \n",
    "    def initialize(self, initialization, std, bias, random_seed):\n",
    "        np.random.seed(random_seed)\n",
    "        if initialization == \"He\":\n",
    "            for l in range(1, self.num_layers + 1):\n",
    "                self.parameters[\"Theta\" + str(l)] = np.random.randn(self.hidden_units[l], self.hidden_units[l - 1]) \\\n",
    "                                                    * np.sqrt(2. / self.hidden_units[l - 1])\n",
    "                self.parameters[\"theta\" + str(l)] = np.zeros((self.hidden_units[l], 1)) + bias\n",
    "        elif initialization == \"Random\":\n",
    "            for l in range(1, self.num_layers + 1):\n",
    "                self.parameters[\"Theta\" + str(l)] = np.random.randn(self.hidden_units[l], self.hidden_units[l - 1]) * std\n",
    "                self.parameters[\"theta\" + str(l)] = np.zeros((self.hidden_units[l], 1)) + bias\n",
    "        else:\n",
    "            raise \"NameError\", \"Undefined Initialization\"\n",
    "    \n",
    "    def mini_batch_split(self, X, y, mini_batch_size, random_seed):\n",
    "        np.random.seed(random_seed)\n",
    "        mini_batches = []\n",
    "        X = X.T\n",
    "        num_observations = X.shape[-1]\n",
    "        order = list(np.random.permutation(num_observations))\n",
    "        X_shuffled = X[:, order]\n",
    "        y_shuffled = y[order].reshape((1, num_observations))\n",
    "        num_full_minibatches = int(np.floor(num_observations * 1.0 / mini_batch_size))\n",
    "        for minibatch in range(num_full_minibatches):\n",
    "            mini_batches.append((X_shuffled[:, minibatch * mini_batch_size: (minibatch + 1) * mini_batch_size],\n",
    "                                 y_shuffled[:, minibatch * mini_batch_size: (minibatch + 1) * mini_batch_size]))\n",
    "        if num_observations % mini_batch_size != 0:\n",
    "            mini_batches.append((X_shuffled[:, num_full_minibatches * mini_batch_size:],\n",
    "                                 y_shuffled[:, num_full_minibatches * mini_batch_size:]))\n",
    "        return mini_batches\n",
    "\n",
    "    def forward_propagation(self, X, y):\n",
    "        self.caches[\"A\" + str(0)] = X\n",
    "        for l in range(1, self.num_layers + 1):\n",
    "            self.caches[\"Z\" + str(l)] = np.dot(self.parameters[\"Theta\" + str(l)], self.caches[\"A\" + str(l - 1)]) \\\n",
    "                                        + self.parameters[\"theta\" + str(l)]\n",
    "            \n",
    "            self.caches[\"A\" + str(l)] = self.activations[\"activation\" + str(l)].evaluation(self.caches[\"Z\" + str(l)])\n",
    "            \n",
    "            if l in self.dropout:\n",
    "                self.caches[\"D\" + str(l)] = (np.random.rand(self.caches[\"A\" + str(l)].shape[0], \n",
    "                                                            self.caches[\"A\" + str(l)].shape[1])) < self.keep_prob\n",
    "                self.caches[\"A\" + str(l)] *=  (self.caches[\"D\" + str(l)] / self.keep_prob)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def backward_propagation(self, X, y):\n",
    "        num_observations = X.shape[-1]\n",
    "        self.caches[\"dA\" + str(self.num_layers)] = self.loss_function.derivative(self.caches[\"A\" + str(self.num_layers)], y)\n",
    "        for l in range(self.num_layers, 0, -1):\n",
    "            self.caches[\"dZ\" + str(l)] = self.caches[\"dA\" + str(l)] * self.activations[\"activation\" + str(l)].derivative(self.caches[\"A\" + str(l)])\n",
    "            self.caches[\"dTheta\" + str(l)] = 1.0 / num_observations * np.dot(self.caches[\"dZ\" + str(l)], self.caches[\"A\" + str(l - 1)].T) + self.regularizer.derivative(self.parameters, l, num_observations, \n",
    "                                                                         self.regularization_coefficient)\n",
    "            \n",
    "            self.caches[\"dtheta\" + str(l)] = 1.0 / num_observations * np.sum(self.caches[\"dZ\" + str(l)], axis = 1, keepdims = True)\n",
    "            \n",
    "            self.caches[\"dA\" + str(l - 1)] = np.dot(self.parameters[\"Theta\" + str(l)].T, self.caches[\"dZ\" + str(l)])\n",
    "            \n",
    "            \n",
    "            if l - 1 in self.dropout:\n",
    "                self.caches[\"dA\" + str(l - 1)] *= (self.caches[\"D\" + str(l - 1)] / self.keep_prob)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def optimize(self, optimizer, learning_rate, epoch, decay_for_avg_grad, decay_for_squared_grad, epsilon):\n",
    "        \n",
    "        if optimizer == \"GD\":\n",
    "            for l in range(1, self.num_layers + 1):\n",
    "                self.parameters[\"Theta\" + str(l)] -= learning_rate * self.caches[\"dTheta\" + str(l)]\n",
    "                self.parameters[\"theta\" + str(l)] -= learning_rate * self.caches[\"dtheta\" + str(l)]\n",
    "                \n",
    "        elif optimizer == \"Adam\":\n",
    "            avg_grad = {}\n",
    "            squared_grad = {}\n",
    "            for l in range(1, self.num_layers + 1):\n",
    "                avg_grad['dTheta' + str(l)] = np.zeros_like(self.parameters['Theta' + str(l)])\n",
    "                avg_grad['dtheta' + str(l)] = np.zeros_like(self.parameters['theta' + str(l)])\n",
    "                squared_grad['dTheta' + str(l)] = np.zeros_like(self.parameters['Theta' + str(l)])\n",
    "                squared_grad['dtheta' + str(l)] = np.zeros_like(self.parameters['theta' + str(l)])\n",
    "            for l in range(1, self.num_layers + 1):\n",
    "                avg_grad['dTheta' + str(l)] = decay_for_avg_grad * avg_grad['dTheta' + str(l)] + (1 - decay_for_avg_grad) * self.caches[\"dTheta\" + str(l)]\n",
    "                \n",
    "                avg_grad['dtheta' + str(l)] = decay_for_avg_grad * avg_grad['dtheta' + str(l)] + (1 - decay_for_avg_grad) * self.caches[\"dtheta\" + str(l)]\n",
    "                \n",
    "                squared_grad['dTheta' + str(l)] = decay_for_squared_grad * squared_grad['dTheta' + str(l)] + (1 - decay_for_squared_grad) * self.caches[\"dTheta\" + str(l)] ** 2\n",
    "                \n",
    "                squared_grad['dtheta' + str(l)] = decay_for_squared_grad * squared_grad['dtheta' + str(l)] + (1 - decay_for_squared_grad) * self.caches[\"dtheta\" + str(l)] ** 2\n",
    "                avg_grad['dTheta' + str(l)] /= (1 - decay_for_avg_grad ** epoch)\n",
    "                avg_grad['dtheta' + str(l)] /= (1 - decay_for_avg_grad ** epoch)\n",
    "                \n",
    "                squared_grad['dTheta' + str(l)] /= (1 - decay_for_squared_grad ** epoch)\n",
    "                squared_grad['dtheta' + str(l)] /= (1 - decay_for_squared_grad ** epoch)\n",
    "                \n",
    "                self.parameters[\"Theta\" + str(l)] -= learning_rate * avg_grad['dTheta' + str(l)] / \\\n",
    "                                                    (np.sqrt(squared_grad['dTheta' + str(l)]) + epsilon)\n",
    "                \n",
    "                self.parameters[\"theta\" + str(l)] -= learning_rate * avg_grad['dtheta' + str(l)] / \\\n",
    "                                                    (np.sqrt(squared_grad['dtheta' + str(l)]) + epsilon)\n",
    "                \n",
    "            \n",
    "    def predicting(self, X):\n",
    "        self.caches[\"A\" + str(0)] = X.T\n",
    "        for l in range(1, self.num_layers + 1):\n",
    "            self.caches[\"Z\" + str(l)] = np.dot(self.parameters[\"Theta\" + str(l)], self.caches[\"A\" + str(l - 1)]) \\\n",
    "                                        + self.parameters[\"theta\" + str(l)]\n",
    "            self.caches[\"A\" + str(l)] = self.activations[\"activation\" + str(l)].evaluation(self.caches[\"Z\" + str(l)])\n",
    "        return (self.caches[\"A\" + str(self.num_layers)] > 0.5)\n",
    "        \n",
    "    def plot_loss(self):\n",
    "        plt.plot(self.loss_collection)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = datasets.load_breast_cancer().data\n",
    "X = (X - X.mean(axis = 0)) / X.std(axis = 0)\n",
    "y = datasets.load_breast_cancer().target\n",
    "num_obs = len(X)\n",
    "training_percentage = 0.8\n",
    "num_training = int(num_obs * training_percentage)\n",
    "X_train, y_train, X_test, y_test = X[:num_training], y[:num_training], X[num_training: ], y[num_training: ] \n",
    "nn = NeuralNetwork(3, [7, 5, 1], [\"relu\", \"relu\", \"sigmoid\"], dropout = [1,2], keep_prob = 0.9, loss=\"logistic\")\n",
    "nn.training(X_train, y_train, learning_rate = 10e-6, epsilon=  10e-8, optimizer=\"Adam\", mini_batch_size=128, \n",
    "            random_seed=22, num_epochs=3000, decay_rate = 0.0)\n",
    "nn.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9824175824175824"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(nn.predicting(X_train) == y_train).sum() * 1.0 / len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9912280701754386"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(nn.predicting(X_test) == y_test).sum() * 1.0 / len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn = NeuralNetwork(1, [1], [\"sigmoid\"])\n",
    "nn.training(X_train, y_train, learning_rate = 10e-5, num_epochs = 5000, optimizer = \"GD\")\n",
    "nn.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.945054945054945"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(nn.predicting(X_train) == y_train).sum() * 1.0 / len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.956140350877193"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(nn.predicting(X_test) == y_test).sum() * 1.0 / len(y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
