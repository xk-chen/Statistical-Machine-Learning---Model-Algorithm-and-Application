{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Suppose we have data $(X_1,Y_1),(X_2,Y_2),\\dots,(X_n,Y_n)$. We want to use a linear model to describe the relationship between $X$ and $Y$.\n",
    "\n",
    "Assume the linear model as follow:\n",
    "\n",
    "1.$X$ is arbitrary, it means $X$ can be fixed or random.\n",
    "\n",
    "2.$Y=\\beta_{0}+\\beta_{1}X+\\epsilon$.\n",
    "\n",
    "3.$\\mathbb{E}[\\epsilon\\mid X = x]=0$, $\\mathbb{V}[\\epsilon\\mid X=x]=\\sigma^2$ for all $x$.\n",
    "\n",
    "4.$\\epsilon$ is independent across observations.\n",
    "\n",
    "Denote regression function $m(X)=\\beta_{0}+\\beta_{1}X$. To measure the quality of regression function, we use **Mean squared error** $MSE(\\beta_{0},\\beta_{1})$. And\n",
    "\n",
    "$$MSE(\\beta_{0},\\beta_{1})= \\mathbb{E}[(Y-(\\beta_{0}+\\beta_{1}X))^2]$$\n",
    "\n",
    "To obtain the optimal estimation, we need to minimize $MSE$.\n",
    "\n",
    "$$\\frac{\\partial\\mathbb{E}[(Y-(\\beta_{0}+\\beta_{1}X))^2]}{\\partial \\beta_{0}}=2(-\\mathbb{E}[Y]+\\beta_{0} + \\beta_{1}\\mathbb{E}[X])$$\n",
    "\n",
    "$$\\frac{\\partial\\mathbb{E}[(Y-(\\beta_{0}+\\beta_{1}X))^2]}{\\partial \\beta_{1}}=2\\big(-\\text{Cov}[X,Y]-\\mathbb{E}[X]\\mathbb{E}[Y]+\\beta_{0}\\mathbb{E}[X]+\\beta_{1}\\mathbb{V}[X]+\\beta_{1}(\\mathbb{E}[X])^2\\big)$$\n",
    "\n",
    "Set the derivative to zero,\n",
    "\n",
    "$$\\hat{\\beta_{1}}=\\frac{\\text{Cov}[X,Y]}{\\mathbb{V}[X]}$$\n",
    "\n",
    "$$\\hat{\\beta_{0}}=\\mathbb{E}[Y]-\\hat{\\beta_{1}}\\mathbb{E}[X]$$\n",
    "\n",
    "Return to our scenario:\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\beta_{1}}&=\\frac{\\text{Cov}[X,Y]}{\\mathbb{V}[X]}\\\\\n",
    "&=\\frac{\\frac{1}{n}\\sum_iX_iY_i-\\bar{X}\\bar{Y}}{s_X^2}\\\\\n",
    "&=\\frac{\\frac{1}{n}\\sum_iX_i(\\beta_{0}+\\beta_{1}X_i+\\epsilon_i)-\\bar{X}(\\beta_{0}+\\beta_{1}X_i+\\epsilon_i)}{s_X^2}\\\\\n",
    "&=\\frac{1}{nS_X^2}\\sum_i(\\beta_{0}X_i+\\beta_{1}X_i^2+X_i\\epsilon_i-\\beta_{0}\\bar{X}-\\beta_{1}X_i\\hat{X}-\\bar{X}\\epsilon_i)\\\\\n",
    "&=\\frac{1}{s_X^2}\\big[[\\beta_{1}(\\frac{1}{n}\\sum_iX_i^2-(\\sum_iX_i)^2)]+\\frac{1}{n}\\sum_i(X_i-\\bar{X})\\epsilon_i\\big]\\\\\n",
    "&=\\beta_{1}+\\sum_i\\frac{X_i-\\bar{X}}{nS_X^2}\\epsilon_i\n",
    "\\end{align*}\n",
    "\n",
    "Note the sample variance $S_X^2=\\frac{1}{n}\\sum_i(X_i-\\bar{X})^2$.\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\beta_{0}}&=\\mathbb{E}[Y]-\\hat{\\beta_{1}}\\mathbb{E}[X]\\\\\n",
    "&=\\frac{1}{n}\\sum_i(\\beta_{0}+\\beta_{1}X_i+\\epsilon_i)-\\hat{\\beta_{1}}\\bar{X}\\\\\n",
    "&=\\beta_{0}+\\beta_{1}\\bar{X}+\\frac{1}{n}\\sum_i\\epsilon_i-\\beta_{1}\\bar{X}-\\bar{X}\\sum_i\\frac{X_i -\\bar{X}}{nS_X^2}\\epsilon_i\\\\\n",
    "&=\\beta_{0}+\\sum_i[\\frac{1}{n}-\\frac{\\bar{X}(X_i-\\bar{X})}{nS_X^2}]\\epsilon_i\n",
    "\\end{align*}\n",
    "\n",
    "As for the estimation of regression function $\\hat{m}(X)$,\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{m}(X)&=\\hat{\\beta_{0}}+\\hat{\\beta_{1}}X\\\\\n",
    "&=\\beta_{0}+\\beta_{1}X+\\sum_i\\big(\\frac{1}{n}+(X-\\bar{X})\\frac{(X_i-\\bar{X})}{nS_X^2}\\big)\\epsilon_i\n",
    "\\end{align*}\n",
    "\n",
    "Then define the residual at $X_j$ as $e_j$, therefore\n",
    "\n",
    "\\begin{align*}\n",
    "e_j&=Y_j-\\hat{m}(X_j)\\\\\n",
    "&=\\beta_{0}+\\beta_{1}X_j+\\epsilon_j-m(X_j)-\\sum_i\\big(\\frac{1}{n}+(X_j-\\bar{X})\\frac{(X_i-\\bar{X})}{nS_X^2}\\big)\\epsilon_i\\\\\n",
    "&=\\epsilon_j-\\sum_i\\big(\\frac{1}{n}+(X_j-\\bar{X})\\frac{(X_i-\\bar{X})}{nS_X^2}\\big)\\epsilon_i\\\\\n",
    "&=\\sum_i\\big(\\delta_{ij}-\\frac{1}{n}-(X_j-\\bar{X})\\frac{(X_i-\\bar{X})}{nS_X^2}\\big)\\epsilon_i\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n",
    "Have a look at the estimation of $\\sigma^2$:\n",
    "\n",
    "$$\\hat{\\sigma}^2=\\frac{1}{n}\\sum_je_j^2$$\n",
    "\n",
    "Let\n",
    "\n",
    "$$c_{ij}=\\delta_{ij}-\\frac{1}{n}-(X_j-\\bar{X})\\frac{(X_i-\\bar{X})}{nS_X^2}$$\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[e_j^2]&=\\mathbb{E}\\big[\\big(\\sum_ic_{ij}\\epsilon_i\\big)^2\\big]\\\\\n",
    "&=\\mathbb{V}\\big[\\sum_ic_{ij}\\epsilon_i\\big]+\\big(\\mathbb{E}\\big[\\sum_ic_{ij}\\epsilon_i\\big]\\big)^2\\\\\n",
    "&=\\mathbb{V}\\big[\\sum_ic_{ij}\\epsilon_i\\big]+\\big(\\sum_ic_{ij}\\mathbb{E}\\epsilon_i\\big)^2\\\\\n",
    "&=\\text{Cov}\\big(\\sum_ic_{ij}\\epsilon_i,\\sum_kc_{kj}\\epsilon_k\\big)\\\\ \n",
    "&=\\sum_ic_{ij}\\sum_kc_{kj}\\text{Cov}\\big(\\epsilon_i,\\epsilon_k\\big)\\\\\n",
    "&=\\sum_ic_{ij}^2\\text{Cov}\\big(\\epsilon_i,\\epsilon_i\\big)\\\\\n",
    "&=\\sum_ic_{ij}^2\\mathbb{V}[\\epsilon_i]\\\\\n",
    "&=\\sigma^2\\sum_ic_{ij}^2\\\\\n",
    "&=\\sigma^2\\sum_i\\big(\\delta_{ij}-\\frac{1}{n}-(X_j-\\bar{X})\\frac{(X_i-\\bar{X})}{nS_X^2}\\big)^2\\\\\n",
    "&=\\sigma^2\\sum_i\\big(\\delta_{ij}^2-\\frac{2}{n}\\delta_{ij}-2\\delta_{ij}(X_j-\\bar{X})\\frac{(X_i-\\bar{X})}{nS_X^2}+\\frac{1}{n^2}+\\frac{2}{n}(X_j-\\bar{X})\\frac{(X_i-\\bar{X})}{nS_X^2}+(X_j-\\bar{X})^2\\frac{(X_i-\\bar{X})^2}{n^2S_X^4}\\big)\\\\\n",
    "&=\\sigma^2\\big(1-\\frac{1}{n}-2\\frac{(X_j-\\bar{X})^2}{nS_X^2}\\big)+\\sigma^2\\sum_i\\big(\\frac{2(X_i-\\bar{X})(X_j-\\bar{X})}{nS_X^2}+\\frac{(X_i-\\bar{X})^2(X_j-\\bar{X})^2}{n^2S_X^4}\\big)\\\\\n",
    "&=\\sigma^2\\big(1-\\frac{1}{n}-2\\frac{(X_j-\\bar{X})^2}{nS_X^2}\\big)+\\frac{2(X_j-\\bar{X})\\sigma^2}{nS_X^2}\\sum_i(X_i-\\bar{X})+\\frac{(X_j-\\bar{X})^2\\sigma^2}{n^2S_X^4}\\sum_i(X_i-\\bar{X})^2\\\\\n",
    "&=\\sigma^2\\big(1-\\frac{1}{n}-2\\frac{(X_j-\\bar{X})^2}{nS_X^2}\\big)+\\frac{(X_j-\\bar{X})^2\\sigma^2}{nS_X^2}\\\\\n",
    "&=\\sigma^2\\big(1-\\frac{1}{n}-\\frac{(X_j-\\bar{X})^2}{nS_X^2}\\big)\n",
    "\\end{align*}\n",
    "\n",
    "Therefore,\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[\\hat{\\sigma}^2]&=\\mathbb{E}\\big[\\frac{1}{n}\\sum_je_j^2\\big]\\\\\n",
    "&=\\frac{1}{n}\\sum_j\\mathbb{E}\\big[e_j^2\\big]\\\\\n",
    "&=\\frac{1}{n}\\sum_j\\sigma^2\\big(1-\\frac{1}{n}-\\frac{(X_j-\\bar{X})^2}{nS_X^2}\\big)\\\\\n",
    "&=\\sigma^2-\\frac{\\sigma^2}{n}-\\frac{\\sigma^2}{nS_X^2}\\sum_j\\frac{(X_j-\\bar{X})^2}{n}\\\\\n",
    "&=\\frac{n-2}{n}\\sigma^2\n",
    "\\end{align*}\n",
    "\n",
    "Then we know \n",
    "\n",
    "$$\\tilde{\\sigma}^2=\\frac{1}{n-2}\\sum_je_j^2$$\n",
    "\n",
    "is the unbiased estimator of $\\sigma^2$ if the linear model assumptions hold.\n",
    "\n",
    "Note that the residuals are related to the noise,\n",
    "\n",
    "$$\\epsilon_i=Y_i-(\\beta_{0}+\\beta_{1}X_i)$$\n",
    "\n",
    "$$e_i=Y_i-(\\hat{\\beta_{0}}-\\hat{\\beta_{1}}X_i)=(\\beta_{0}-\\hat{\\beta_{0}})+(\\beta_{1}-\\hat{\\beta_{1}})X_i+\\epsilon_i$$\n",
    "\n",
    "In general\n",
    "\n",
    "$$\\sum_i\\epsilon_i\\not=0$$\n",
    "\n",
    "while\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_ie_i&=\\sum_iY_i-(\\hat{\\beta_{0}}-\\hat{\\beta_{1}}X_i)\\\\\n",
    "&=\\sum_iY_i-(\\bar{Y}-\\hat{\\beta_{1}}\\bar{X})-\\hat{\\beta_{1}}X_i\\\\\n",
    "&=\\sum_i(Y_i-\\bar{Y})+\\sum_i(\\hat{\\beta_{1}}\\bar{X}-\\hat{\\beta_{1}}X_i)\\\\\n",
    "&=0\n",
    "\\end{align*}\n",
    "\n",
    "It means that although $\\epsilon$'s are independent of each other, it does not hold for residuals. It is the same that $\\sum_i\\epsilon_i(X_i-\\bar{X})\\not=0$ while $\\sum_ie_i(X_i-\\bar{X})=0$.\n",
    "\n",
    "To sum up,\n",
    "\n",
    "1.The residuals should have expectation zero conditional on $X$. And the sample mean of $e_i$ is exactly $0$.\n",
    "\n",
    "2.The residuals show a constant variance, unchanging with $X$.\n",
    "\n",
    "3.The residuals are correlated, but the correlation is extremely weak."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian-noise Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above results derived from current assumptions give us information about the expectation and variance of the estimations, however, it can not tell us the distribution or confidence interval of them. To get stronger conclusion, we need stronger assumption.\n",
    "\n",
    "Here is the Gaussian-noise simple linear regression model:\n",
    "\n",
    "1.The distribution of $X$ is arbitrary, it means $X$ can be fixed or random.\n",
    "\n",
    "2.$Y=\\beta_{0}+\\beta_{1}X+\\epsilon$.\n",
    "\n",
    "3.$\\epsilon\\sim\\mathcal{N}(0,\\sigma^2)$, and is independent of $X$.\n",
    "\n",
    "4.$\\epsilon$ is independent across observations.\n",
    "\n",
    "Having these assumptions, we are able to use the method of maximum likelihood(same as LSE), we obtain\n",
    "\n",
    "$$\\hat{\\beta_1}=\\frac{c_{XY}}{S_X^2}=\\beta_{1}+\\sum_i\\frac{X_i-\\bar{X}}{nS_X^2}\\epsilon_i$$\n",
    "\n",
    "$$\\hat{\\beta_0}=\\bar{Y}-\\hat{\\beta_1}\\bar{X}=\\beta_{0}+\\sum_i[\\frac{1}{n}-\\frac{\\bar{X}(X_i-\\bar{X})}{nS_X^2}]\\epsilon_i$$\n",
    "\n",
    "$$\\hat{\\sigma}^2=\\frac{1}{n}\\sum_i(Y_i-(\\hat{\\beta_0}+\\hat{\\beta_1}X_i))^2$$\n",
    "\n",
    "From the normal assumption:\n",
    "\n",
    "$$\\hat{\\beta_1}\\sim\\mathcal{N}(\\beta_1,\\frac{\\sigma^2}{nS_X^2})$$\n",
    "\n",
    "$$\\hat{m}(X)\\sim\\mathcal{N}\\big(\\beta_0+\\beta_1X,\\frac{\\sigma^2}{n}(1+\\frac{X-\\bar{X}}{S_X^2})\\big)$$\n",
    "\n",
    "For the statistical inference, we have\n",
    "\n",
    "$$\\frac{\\hat{\\beta_1}-\\beta_1}{\\sigma \\big/\\sqrt{nS_X^2}}\\sim\\mathcal{N}(0,1)$$\n",
    "\n",
    "And by the same method\n",
    "\n",
    "$$\\frac{\\hat{\\beta_0}-\\beta_0}{\\sqrt{\\frac{\\sigma^2}{n}\\big(1+\\frac{\\bar{X}^2}{S_X^2}\\big)}}\\sim\\mathcal{N}(0,1)$$\n",
    "\n",
    "We have shown that\n",
    "\n",
    "$$\\mathbb{E}[\\hat{\\sigma}^2]=\\frac{n-2}{n}\\sigma^2$$\n",
    "\n",
    "Adding on normal assumption\n",
    "\n",
    "$$\\frac{n\\hat{\\sigma}^2}{\\sigma^2}\\sim\\chi_{n-2}^2$$\n",
    "\n",
    "If we have prior $\\sigma$, it is easy to compute $\\text{se}[\\hat{\\beta_1}]$ and $\\text{se}[\\hat{\\beta_0}]$. We can claim that with probability $1-\\alpha$,\n",
    "\n",
    "$$\\beta_1\\in \\big[\\hat{\\beta_1}-\\text{se}[\\hat{\\beta_1}]z_{\\alpha/2},\\hat{\\beta_1}-\\text{se}[\\hat{\\beta_1}]z_{1-\\alpha/2}\\big]$$\n",
    "\n",
    "$$\\beta_0\\in \\big[\\hat{\\beta_0}-\\text{se}[\\hat{\\beta_0}]z_{\\alpha/2},\\hat{\\beta_0}-\\text{se}[\\hat{\\beta_0}]z_{1-\\alpha/2}\\big]$$\n",
    "\n",
    "Unfortunately, the value of $\\sigma$ is unknown, so we can not calculate the left hand. Then we come up with the estimation of $\\text{se}[\\hat{\\beta_1}]$ and $\\text{se}[\\hat{\\beta_0}]$, which is called **estimation of standard deviation**. Where\n",
    "\n",
    "$$\\hat{\\text{se}}[\\hat{\\beta_1}]=\\frac{\\sigma}{S_X\\sqrt{n-2}} $$\n",
    "\n",
    "$$\\hat{\\text{se}}[\\hat{\\beta_0}]=\\frac{\\sigma}{S_X\\sqrt{n-2}}\\sqrt{S_X^2+\\bar{X}^2}$$\n",
    "\n",
    "and $n-2$ is the de-biased version. So what about the sampling distribution of $\\frac{\\hat{\\beta}-\\beta}{\\hat{\\text{se}}[\\hat{\\beta}]}$?\n",
    "\n",
    "\\begin{proposition}\n",
    "\n",
    "If $Z\\sim\\mathcal{N}(0,1)$, $S^2\\sim\\chi_d^2$, and $Z$ and $S^2$ are independent, then\n",
    "\n",
    "$$\\frac{Z}{\\sqrt{S^2\\big/d}}\\sim t_d$$\n",
    "\n",
    "\\end{proposition}\n",
    "\n",
    "Then \n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\hat{\\beta_1}-\\beta_1}{\\hat{\\text{se}}[\\hat{\\beta_1}]}&=\\frac{\\hat{\\beta_1}-\\beta_1}{\\sigma}\\frac{\\sigma}{\\hat{\\text{se}}[\\hat{\\beta_1}]}\\\\\n",
    "&=\\frac{\\frac{\\hat{\\beta_1}-\\beta_1}{\\sigma}}{\\frac{\\hat{\\text{se}}[\\hat{\\beta_1}]}{\\sigma}}\\\\\n",
    "&=\\frac{\\mathcal{N}(0,\\frac{1}{nS_X^2})}{\\frac{\\hat{\\sigma}}{S_X\\sigma\\sqrt{n-2}}}\\\\\n",
    "&=\\frac{\\sqrt{n}\\mathcal{N}(0,\\frac{1}{n})}{\\frac{\\sqrt{\\hat{\\sigma}}}{\\sigma\\sqrt{n-2}}}\\\\\n",
    "&=\\frac{\\mathcal{N}(0,1)}{\\sqrt{\\chi_{n-2}^2\\big/(n-2)}}\\\\\n",
    "&=t_{n-2}\n",
    "\\end{align*}\n",
    "\n",
    "Hence, \n",
    "\n",
    "$$\\frac{\\hat{\\beta_1}-\\beta_1}{\\hat{\\text{se}}[\\hat{\\beta_1}]}\\sim t_{n-2}$$\n",
    "\n",
    "$$\\frac{\\hat{\\beta_0}-\\beta_0}{\\hat{\\text{se}}[\\hat{\\beta_0}]}\\sim t_{n-2}$$\n",
    "\n",
    "Then the $1-\\alpha$ confidence interval is\n",
    "\n",
    "$$\\beta_1\\in \\big[\\hat{\\beta_1}-\\hat{\\text{se}}[\\hat{\\beta_1}]t_{\\alpha/2}(n-2),\\hat{\\beta_1}-\\hat{\\text{se}}[\\hat{\\beta_1}]t_{1-\\alpha/2}(n-2)\\big]$$\n",
    "\n",
    "$$\\beta_0\\in \\big[\\hat{\\beta_0}-\\hat{\\text{se}}[\\hat{\\beta_0}]t_{\\alpha/2}(n-2),\\hat{\\beta_0}-\\hat{\\text{se}}[\\hat{\\beta_0}]t_{1-\\alpha/2}(n-2)\\big]$$\n",
    "\n",
    "Moreover, we can test the hypothesis if $\\beta_1=0$ using $t$-statistics we obtain from above. Note the asymptotics distribution of $t$-distribution is the standard normal distribution.\n",
    "\n",
    "It is the same as $\\hat{m}(X)$, since $\\hat{\\text{se}}[\\hat{m}(X)]=\\frac{\\hat{\\sigma}}{\\sqrt{n-2}}\\sqrt{1+\\frac{(X-\\bar{X})^2}{S_X^2}}$,\n",
    "the sampling distribution is $t$-distribution, too.\n",
    "\n",
    "$$\\frac{\\hat{m}(X)-m(X)}{\\hat{\\text{se}}[\\hat{m}(X)]}\\sim t_{n-2}$$\n",
    "\n",
    "Note:\n",
    "\n",
    "Transforming the predictor variable raises no issue, while for the response may cause errors. For instance\n",
    "\n",
    "$$g(Y)=\\beta_{0}+\\beta_{1}X+\\epsilon$$\n",
    "\n",
    "We may get\n",
    "\n",
    "$$\\mathbb{E}[g(Y\\mid X=x)]=\\beta_{0}+\\beta_{1}x$$\n",
    "\n",
    "While\n",
    "\n",
    "$$\\mathbb{E}[Y\\mid X=x]\\not=g^{-1}(\\beta_{0}+\\beta_{1}x)$$\n",
    "\n",
    "beacuse of the **convexity** of the function.\n",
    "\n",
    "However we can get the right prediction interval because of the monotonic property.\n",
    "\n",
    "Another quantity $R^2$ is defined as\n",
    "\n",
    "$$R^2=\\frac{\\sum_i[(Y_i-\\bar{Y})^2-(Y_i-\\hat{Y_i})^2]}{\\sum_i(Y_i-\\bar{Y})^2}$$\n",
    "\n",
    "It is often described as the fraction of variability explained by regression. When $n\\rightarrow\\infty$\n",
    "\n",
    "\\begin{align*}\n",
    "R^2&=\\frac{\\mathbb{V}[m(X)]}{\\mathbb{V}[Y]}\\\\\n",
    "&=\\frac{\\mathbb{V}[\\beta_{0}+\\beta_{1}X]}{\\mathbb{V}[\\beta_{0}+\\beta_{1}X+\\epsilon]}\\\\\n",
    "&=\\frac{\\beta_{1}^2\\mathbb{V}[X]}{\\beta_{1}^2\\mathbb{V}[X]+\\sigma^2}\n",
    "\\end{align*}\n",
    "\n",
    "It is clear that $R^2$ does not measure goodness of fit. For that the value is not determined by the model, it may be caused by $\\mathbb{V}[X]$ or $\\sigma^2$. \n",
    "\n",
    "And $R^2$ does not reflect the predictability, for that if we change the range of $X$ we can get any value between $0$ and $1$, for this topic, the $MSE$ is much better. Meanwhile it does not give us information about interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "When we have more than one predictors, the multiple linear regression model is\n",
    "\n",
    "1.There are $p$ quantitative predictors $X_1,X_2,\\dots,X_p$. We make no assumptions about their distribution. And the variables may dependent. Denote $X=(X_1,X_2\\dots,X_p)$.\n",
    "\n",
    "2.There is a single response vaiable $Y$.\n",
    "\n",
    "3.The linear model is $Y=\\beta_{0}+\\sum_{i=1}^p\\beta_{i}X_i+\\epsilon$.\n",
    "\n",
    "4.The noise variable $\\epsilon$ satisfies $\\mathbb{E}[\\epsilon\\mid X = x]=0$ and $\\mathbb{V}[\\epsilon\\mid X=x]=\\sigma^2$, and uncorrelated across observations.\n",
    "\n",
    "Generally, the matrix form of $n$ observations is\n",
    "\n",
    "$$\\mathbf{Y}_{n\\times 1}=\\mathbf{X}_{n\\times(p+1)}\\beta_{(p+1)\\times 1}$$\n",
    "\n",
    "The first column of $\\mathbf{X}$ is all $1$'s, and $\\mathbb{E}[\\epsilon\\mid \\mathbf{X}]=0$, $\\mathbb{V}[\\epsilon\\mid \\mathbf{X}]=\\sigma^2\\mathbf{I}$.\n",
    "\n",
    "We now wish to estimate the model by least squares, then the $MSE$ is\n",
    "\n",
    "$$MSE(\\beta)=\\frac{1}{n}(\\mathbf{Y}-\\mathbf{X}\\beta)^T(\\mathbf{Y}-\\mathbf{X}\\beta)$$\n",
    "\n",
    "And the gradient is\n",
    "\n",
    "$$\\nabla_{\\beta}MSE(\\beta)=\\frac{2}{n}(-\\mathbf{X}^T\\mathbf{Y}+\\mathbf{X}^T\\mathbf{X}\\beta)$$\n",
    "\n",
    "Set the gradient as $0$, the optimal estimation is\n",
    "\n",
    "$$\\hat{\\beta}=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T(\\mathbf{X}\\beta+\\epsilon)=\\beta+(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\epsilon$$\n",
    " \n",
    "The property of estimation:\n",
    "\n",
    "$$\\mathbb{E}[\\hat{\\beta}]=\\beta$$\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{V}[\\hat{\\beta}]&=\\mathbb{V}[\\beta+(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\epsilon]\\\\\n",
    "&=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbb{V}[\\epsilon]\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\\n",
    "&=(\\mathbf{X}^T\\mathbf{X})^{-1}\\sigma^2\\\\\n",
    "\\end{align*}\n",
    "\n",
    "The estimation of $\\mathbf{Y}$ is\n",
    "\n",
    "$$\\hat{\\mathbf{Y}}=\\mathbf{X}\\hat{\\beta}=\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}=\\mathbf{H}\\mathbf{Y}$$\n",
    "\n",
    "where $\\mathbf{H}$ projects $\\mathbf{Y}$ onto the column space of $\\mathbf{X}$.\n",
    "\n",
    "$$\\mathbb{E}[\\hat{\\mathbf{Y}}]=\\mathbb{E}[\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T(\\mathbf{X}\\beta+\\epsilon)]=\\mathbf{X}\\beta$$\n",
    "\n",
    "$$\\mathbb{V}[\\hat{\\mathbf{Y}}]=\\mathbb{V}[\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\epsilon]=\\mathbb{V}[\\mathbf{H}\\epsilon]=\\mathbf{H}\\sigma^2$$\n",
    "\n",
    "Residuals are given by\n",
    "\n",
    "$$\\mathbf{e}=\\mathbf{Y}-\\hat{\\mathbf{Y}}=(\\mathbf{I}-\\mathbf{H})\\epsilon$$\n",
    "\n",
    "Therefore, \n",
    "\n",
    "$$\\mathbb{E}[\\mathbf{e}]=\\mathbf{0}$$\n",
    "\n",
    "$$\\mathbb{V}[\\mathbf{e}]=\\sigma^2(\\mathbf{I}-\\mathbf{H})$$\n",
    "\n",
    "Note that both $\\mathbf{H}$ and $\\mathbf{I}-\\mathbf{H}$ are symmetric idempotent matrices.\n",
    "\n",
    "If the assumptions hold, we can estimate the variance $\\sigma^2$ using\n",
    "\n",
    "$$\\hat{\\sigma}^2=\\frac{1}{n}\\mathbf{e}^T\\mathbf{e}$$\n",
    "\n",
    "Moreover, \n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[\\hat{\\sigma}^2]&=\\frac{1}{n}\\mathbb{E}[\\mathbf{e}^T\\mathbf{e}]\\\\\n",
    "&=\\frac{1}{n}\\mathbb{E}[((\\mathbf{I}-\\mathbf{H})\\epsilon)^T((\\mathbf{I}-\\mathbf{H})\\epsilon)]\\\\\n",
    "&=\\frac{1}{n}\\mathbb{E}[\\epsilon^T(\\mathbf{I}-\\mathbf{H})\\epsilon]\\\\\n",
    "&=\\frac{1}{n}\\mathbb{E}[\\text{tr}((\\mathbf{I}-\\mathbf{H})\\epsilon\\epsilon^T)]\\\\\n",
    "&=\\frac{1}{n}\\text{tr}((\\mathbf{I}-\\mathbf{H})\\mathbb{V}[\\epsilon])\\\\\n",
    "&=\\frac{\\sigma^2}{n}\\text{tr}\\big(\\mathbf{I}-\\mathbf{H}\\big)\\\\\n",
    "&=\\frac{\\sigma^2}{n}(n-\\text{tr}\\mathbf{H})\\\\\n",
    "&=\\frac{\\sigma^2}{n}\\Big(n-\\text{tr}\\big(\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\big)\\Big)\\\\\n",
    "&=\\frac{\\sigma^2}{n}\\Big(n-\\text{tr}\\big(\\mathbf{I}_{(p+1)\\times (p+1)}\\big)\\Big)\\\\\n",
    "&=\\frac{n-p-1}{n}\\sigma^2\n",
    "\\end{align*}\n",
    "\n",
    "To obtain the good statistical property, add another assumption $\\epsilon\\sim\\mathcal{N}(\\mathbf{0},\\sigma^2\\mathbf{I})$.\n",
    "\n",
    "Then it is easy to derive the distributions of the estimators:\n",
    "\n",
    "$$\\hat{\\beta}\\sim\\mathcal{N}(\\beta,\\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1})$$\n",
    "\n",
    "$$\\hat{\\mathbf{Y}}\\sim\\mathcal{N}(\\mathbf{X}\\beta,\\sigma^2\\mathbf{H})$$\n",
    "\n",
    "$$\\hat{\\mathbf{e}}\\sim\\mathcal{N}(0,\\sigma^2(\\mathbf{I}-\\mathbf{H}))$$\n",
    "\n",
    "$$\\frac{n\\hat{\\sigma}^2}{\\sigma^2}\\sim \\chi_{n-p-1}^2$$\n",
    "\n",
    "For the $j$-th covariate, without knowing the true variance $\\sigma^2$, we have\n",
    "\n",
    "$$\\frac{\\hat{\\beta_j}-\\beta_j}{\\hat{\\text{se}_j}}\\sim t_{n-p-1}$$\n",
    "\n",
    "where $\\hat{\\text{se}_j}=\\sqrt{\\hat{\\sigma}^2(\\mathbf{X}^T\\mathbf{X})^{-1}_{j+1,j+1}}$. Therefore we can test if $X_j$ matters $Y$ by $H_0:\\beta_j=0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'Air.Flow'</li>\n",
       "\t<li>'Water.Temp'</li>\n",
       "\t<li>'Acid.Conc.'</li>\n",
       "\t<li>'stack.loss'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'Air.Flow'\n",
       "\\item 'Water.Temp'\n",
       "\\item 'Acid.Conc.'\n",
       "\\item 'stack.loss'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'Air.Flow'\n",
       "2. 'Water.Temp'\n",
       "3. 'Acid.Conc.'\n",
       "4. 'stack.loss'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"Air.Flow\"   \"Water.Temp\" \"Acid.Conc.\" \"stack.loss\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data(stackloss)\n",
    "names(stackloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nAEQCKAAiARQAkQAKgEgABfgPdmMGvzxwZCIAAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "par(fin = c(10,10))\n",
    "pairs(stackloss, pch = 19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = stack.loss ~ Air.Flow + Water.Temp + Acid.Conc., \n",
       "    data = stackloss)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-7.2377 -1.7117 -0.4551  2.3614  5.6978 \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept) -39.9197    11.8960  -3.356  0.00375 ** \n",
       "Air.Flow      0.7156     0.1349   5.307  5.8e-05 ***\n",
       "Water.Temp    1.2953     0.3680   3.520  0.00263 ** \n",
       "Acid.Conc.   -0.1521     0.1563  -0.973  0.34405    \n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 3.243 on 17 degrees of freedom\n",
       "Multiple R-squared:  0.9136,\tAdjusted R-squared:  0.8983 \n",
       "F-statistic:  59.9 on 3 and 17 DF,  p-value: 3.016e-09\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model <- lm(stack.loss ~ Air.Flow + Water.Temp + Acid.Conc., data = stackloss)\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>5 %</th><th scope=col>95 %</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>(Intercept)</th><td>-60.6140306</td><td>-19.2253183</td></tr>\n",
       "\t<tr><th scope=row>Air.Flow</th><td>  0.4810400</td><td>  0.9502404</td></tr>\n",
       "\t<tr><th scope=row>Water.Temp</th><td>  0.6550686</td><td>  1.9355036</td></tr>\n",
       "\t<tr><th scope=row>Acid.Conc.</th><td> -0.4240127</td><td>  0.1197676</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & 5 \\% & 95 \\%\\\\\n",
       "\\hline\n",
       "\t(Intercept) & -60.6140306 & -19.2253183\\\\\n",
       "\tAir.Flow &   0.4810400 &   0.9502404\\\\\n",
       "\tWater.Temp &   0.6550686 &   1.9355036\\\\\n",
       "\tAcid.Conc. &  -0.4240127 &   0.1197676\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | 5 % | 95 % | \n",
       "|---|---|---|---|\n",
       "| (Intercept) | -60.6140306 | -19.2253183 | \n",
       "| Air.Flow |   0.4810400 |   0.9502404 | \n",
       "| Water.Temp |   0.6550686 |   1.9355036 | \n",
       "| Acid.Conc. |  -0.4240127 |   0.1197676 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "            5 %         95 %       \n",
       "(Intercept) -60.6140306 -19.2253183\n",
       "Air.Flow      0.4810400   0.9502404\n",
       "Water.Temp    0.6550686   1.9355036\n",
       "Acid.Conc.   -0.4240127   0.1197676"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "confint(model, level = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>fit</th><th scope=col>lwr</th><th scope=col>upr</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>14.41064</td><td>4.759959</td><td>24.06133</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lll}\n",
       "  & fit & lwr & upr\\\\\n",
       "\\hline\n",
       "\t1 & 14.41064 & 4.759959 & 24.06133\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | fit | lwr | upr | \n",
       "|---|\n",
       "| 1 | 14.41064 | 4.759959 | 24.06133 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  fit      lwr      upr     \n",
       "1 14.41064 4.759959 24.06133"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict(model, newdata = data.frame(Air.Flow = 58, Water.Temp = 20, Acid.Conc.=86), interval = \"prediction\", level = 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>Df</th><th scope=col>Sum Sq</th><th scope=col>Mean Sq</th><th scope=col>F value</th><th scope=col>Pr(&gt;F)</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>Air.Flow</th><td> 1          </td><td>1750.121989 </td><td>1750.121989 </td><td>166.3707443 </td><td>3.308729e-10</td></tr>\n",
       "\t<tr><th scope=row>Water.Temp</th><td> 1          </td><td> 130.320772 </td><td> 130.320772 </td><td> 12.3886015 </td><td>2.629043e-03</td></tr>\n",
       "\t<tr><th scope=row>Acid.Conc.</th><td> 1          </td><td>   9.965372 </td><td>   9.965372 </td><td>  0.9473319 </td><td>3.440461e-01</td></tr>\n",
       "\t<tr><th scope=row>Residuals</th><td>17          </td><td> 178.829962 </td><td>  10.519410 </td><td>         NA </td><td>          NA</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllll}\n",
       "  & Df & Sum Sq & Mean Sq & F value & Pr(>F)\\\\\n",
       "\\hline\n",
       "\tAir.Flow &  1           & 1750.121989  & 1750.121989  & 166.3707443  & 3.308729e-10\\\\\n",
       "\tWater.Temp &  1           &  130.320772  &  130.320772  &  12.3886015  & 2.629043e-03\\\\\n",
       "\tAcid.Conc. &  1           &    9.965372  &    9.965372  &   0.9473319  & 3.440461e-01\\\\\n",
       "\tResiduals & 17           &  178.829962  &   10.519410  &          NA  &           NA\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | Df | Sum Sq | Mean Sq | F value | Pr(>F) | \n",
       "|---|---|---|---|\n",
       "| Air.Flow |  1           | 1750.121989  | 1750.121989  | 166.3707443  | 3.308729e-10 | \n",
       "| Water.Temp |  1           |  130.320772  |  130.320772  |  12.3886015  | 2.629043e-03 | \n",
       "| Acid.Conc. |  1           |    9.965372  |    9.965372  |   0.9473319  | 3.440461e-01 | \n",
       "| Residuals | 17           |  178.829962  |   10.519410  |          NA  |           NA | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "           Df Sum Sq      Mean Sq     F value     Pr(>F)      \n",
       "Air.Flow    1 1750.121989 1750.121989 166.3707443 3.308729e-10\n",
       "Water.Temp  1  130.320772  130.320772  12.3886015 2.629043e-03\n",
       "Acid.Conc.  1    9.965372    9.965372   0.9473319 3.440461e-01\n",
       "Residuals  17  178.829962   10.519410          NA           NA"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "anova(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the optimal estimation\n",
    "\n",
    "$$\\hat{\\beta}=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}$$\n",
    "\n",
    "If $(\\mathbf{X}^T\\mathbf{X})^{-1}$ is not invertible, we may not be able to get $\\hat{\\beta}$, and the variance $(\\mathbf{X}^T\\mathbf{X})^{-1}\\sigma^2$ will blow up.\n",
    "\n",
    "Let $\\mathbf{G}=\\mathbf{X}^T\\mathbf{X}$, it is easy to show that\n",
    "\n",
    "1.$\\mathbf{G}$ is $(p+1)\\times(p+1)$.\n",
    "\n",
    "2.$\\mathbf{G}$ is symmetric.\n",
    "\n",
    "3.$\\mathbf{G}$ is positive semi-definite, i.e. $\\mathbf{a}^T\\mathbf{G}\\mathbf{a}\\geq 0$.\n",
    "\n",
    "If there is the phenomena of multicollinearity, $\\exists \\mathbf{a}:\\mathbf{X}\\mathbf{a}=\\mathbf{0}$.Then $\\mathbf{a}^T\\mathbf{G}\\mathbf{a}=\\mathbf{0}$ holds for some $\\mathbf{a}$.\n",
    "\n",
    "According to the eigen-decomposition\n",
    "\n",
    "$$\\mathbf{0}=\\mathbf{a}^T\\mathbf{G}\\mathbf{a}=\\mathbf{a}^T\\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}^T\\mathbf{a}=\\sum_j\\lambda_j(\\mathbf{a}^T\\mathbf{q}_j)^2$$\n",
    "\n",
    "For that $(\\mathbf{a}^T\\mathbf{q}_j)^2$ could not be zero simultaneously. Then we conclude that at least one eigenvalue of $\\mathbf{G}$ is zero. Therefore we can examine if multicollinearity happens by eigen-decomposition.\n",
    "\n",
    "In order to solve the singular problem, we introduce **Ridge Regression**\n",
    "\n",
    "$$MSE(\\beta)=\\frac{1}{n}(\\mathbf{Y}-\\mathbf{X}\\beta)^T(\\mathbf{Y}-\\mathbf{X}\\beta)+\\frac{\\lambda}{n}\\|\\beta\\|^2$$\n",
    "\n",
    "And\n",
    "\n",
    "$$\\nabla_{\\beta}MSE(\\beta)=\\nabla_{\\beta}\\big(\\frac{1}{n}(\\mathbf{Y}-\\mathbf{X}\\beta)^T(\\mathbf{Y}-\\mathbf{X}\\beta)+\\frac{\\lambda}{n}\\beta^T\\beta\\big)=\\frac{2}{n}(-\\mathbf{X}^T\\mathbf{Y}+\\mathbf{X}^T\\mathbf{X}\\beta+\\lambda\\beta)$$\n",
    "\n",
    "$$\\hat{\\beta}_\\lambda=(\\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{Y}$$\n",
    "\n",
    "This estimator is biased...\n",
    "\n",
    "$$\\mathbb{E}[\\hat{\\beta}_\\lambda]=(\\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{I})^{-1}\\mathbf{X}^T\\mathbb{E}[\\mathbf{Y}]=(\\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{X}\\beta$$\n",
    "\n",
    "$$\\mathbb{V}[\\hat{\\beta}_\\lambda]=\\mathbb{V}\\big[(\\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{I})^{-1}\\mathbf{X}^T\\epsilon\\big]=\\sigma^2(\\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{I})^{-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## $F$-test for Multiple Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $S\\subset\\{1,2,\\dots,p\\}$ be the number of null model, and define the full model be the model using all $p$ covariant. We can show that $\\hat{\\sigma}_{null}^2\\geq \\hat{\\sigma}_{full}^2$ always holds. Then we have below null hypothesis\n",
    "\n",
    "$$H_0:\\beta_j=0,j\\in S$$\n",
    "\n",
    "We have shown that under normal assumption\n",
    "\n",
    "$$\\frac{n\\hat{\\sigma}_{full}^2}{\\sigma^2}\\sim\\chi_{n-p-1}^2$$\n",
    "\n",
    "$$\\frac{n\\hat{\\sigma}_{null}^2}{\\sigma^2}\\sim\\chi_{s}^2$$\n",
    "\n",
    "where $s$ is the number of variables in null model, from the property $\\chi^2(\\mu)+\\chi^2(\\nu)=\\chi^2(\\mu+\\nu)$. Therefore,\n",
    "\n",
    "$$\\frac{n(\\hat{\\sigma}_{null}^2-\\hat{\\sigma}_{full}^2)}{\\sigma^2}\\sim\\chi_{p-s}^2$$\n",
    "\n",
    "And define the $F$-statistics\n",
    "\n",
    "$$F=\\frac{(\\hat{\\sigma}_{null}^2-\\hat{\\sigma}_{full}^2)\\big/(p-s)}{\\hat{\\sigma}_{full}^2\\big/(n-p-1)}\\sim F_{p-s,n-p-1}$$\n",
    "\n",
    "Then we reject the null hypothesis when\n",
    "\n",
    "$$F>F_{p-s,n-p-1}(\\alpha)$$\n",
    "\n",
    "In particularly, if the null model is $Y=\\beta_0+\\epsilon$, our $F=\\frac{(\\hat{\\sigma}_{null}^2-\\hat{\\sigma}_{full}^2)/p}{\\hat{\\sigma}_{full}^2/(n-p-1)}\\sim F_{p,n-p-1}$\n",
    "\n",
    "Note: $F$-test does not test any of the following:\n",
    "\n",
    "1.Whether some variable ought to be included in the model.\n",
    "\n",
    "2.Whether the relationship between $Y$ and $X_i$ is linear.\n",
    "\n",
    "3.Whether any the assumptions hold.\n",
    "\n",
    "\n",
    "Alternatively, we can use confidence ellipsoids, where\n",
    "\n",
    "$$(\\hat{\\beta_S}-\\beta_S)^T\\Sigma_S^{-1}(\\hat{\\beta_S}-\\beta_S)\\sim\\chi_s^2$$\n",
    "\n",
    "and the $1-\\alpha$ confidence ellipsoid for $\\beta_S$ is\n",
    "\n",
    "$$C=\\big\\{\\beta_S:(\\hat{\\beta_S}-\\beta_S)^T\\Sigma_S^{-1}(\\hat{\\beta_S}-\\beta_S)\\leq c_\\alpha\\big\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers and Influential Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An **outlier** is a point with a large residual. An **influential point** is a point that has a large impact on the regression. They are totally different things.\n",
    "\n",
    "Define the standardized residual\n",
    "\n",
    "$$r_i=\\frac{e_i}{\\hat{\\sigma}\\sqrt{1-h_{ii}}}$$\n",
    "\n",
    "where $h_{ii}=\\mathbf{H}_{ii}$, and $h_{ii}$, which is called **leverage**, is the contribution of $i$-th data to $\\hat{Y}_i$.\n",
    "\n",
    "To get a better idea of how influential the $i$-th point is, we can fit other model omit this observation. Let $\\mathbf{Y}^{(-i)}$ be the vector if fitted values when we remove data $i$. The **Cook's distance** is defined by:\n",
    "\n",
    "$$D_i=\\frac{(\\mathbf{Y}-\\mathbf{Y}^{(-i)})^T(\\mathbf{Y}-\\mathbf{Y}^{(-i)})}{(p+1)\\hat{\\sigma}^2}=\\frac{r_i^2h_{ii}}{(p+1)(1-h_{ii})}$$\n",
    "\n",
    "Therefore, the influence of a point is determined by both its residual and its leverage. Often when $D_i>1$ we regard $i$ as an influential point.\n",
    "\n",
    "Detect outliers:\n",
    "\n",
    "1.We can look at the leverage, which depends only on the predictors.\n",
    "\n",
    "2.Look at the standardized residuals by how far they are from the regression line.\n",
    "\n",
    "3.Look at the Cook's distance, which say the influence on regression when remove the point.\n",
    "\n",
    "We must be careful when we encounter outliers, they may come from some patterns or caused by noise. We can delete the absurd point by some scientific criteria, or change our model carefully. Another method is called **robust regression**. Usually the robust estimation is based on minimizing a function of the form\n",
    "\n",
    "$$\\hat{\\beta}=\\arg\\min_\\beta\\frac{1}{n}\\sum_{i=1}^n\\rho(y_i-\\mathbf{x}_i^T\\beta)$$\n",
    "\n",
    "The loss function, say $\\rho(\\cdot)$, is varied. $\\rho(u)=\\big|u\\big|$ is the least absolute deviation, and $\\rho(u)=u^2$ is the OLS. A popular compromise  is the **Huber's loss**\n",
    "\n",
    "$$\n",
    "\\rho(u)=\\Big\\{\n",
    "\\begin{align*}\n",
    "& u^2, \\big|u\\big|\\leq c\\\\\n",
    "& 2c\\big|u\\big|, \\big|u\\big|\\geq c\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "However, robust regression is aimed at the linear pattern with large noise, it can not solve non-linearity or correlated noise etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nSUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data(stackloss)\n",
    "model <- lm(stack.loss ~ Air.Flow, data = stackloss)\n",
    "leverage = hatvalues(model)\n",
    "c_statistics = cooks.distance(model)\n",
    "par(mfrow = c(1, 2), pin = c(4, 4))\n",
    "plot(leverage, ylab = \"leverage\", font = 6)\n",
    "plot(c_statistics, ylab = \"Cook's distance\", font = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have multiple models, how to evaluate them? An obvious conclusion is the lower the generalization error is, the better the model. And it is natural to replace it with in-sample $MSE$. Unfortunately, the training error is always smaller than the generalization error. Consider predicting the new data at the same point $X_i$,\n",
    "\n",
    "We have training model\n",
    "\n",
    "$$\\mathbf{Y}=\\mathbf{X}\\beta+\\epsilon$$\n",
    "\n",
    "and the in-sample error is\n",
    "\n",
    "$$\\frac{1}{n}\\sum_{i=1}^n(Y_i-\\hat{Y}_i)^2$$\n",
    "\n",
    "the test model\n",
    "\n",
    "$$\\mathbf{Y}'=\\mathbf{X}\\beta+\\epsilon'$$\n",
    "\n",
    "and the test error\n",
    "\n",
    "$$\\frac{1}{n}\\sum_{i=1}^n(Y'_i-\\hat{Y}_i)^2$$\n",
    "\n",
    "\\begin{theorem}\n",
    "\n",
    "The training error is always smaller than the generalization error.\n",
    "\n",
    "$$\\mathbb{E}\\big[\\frac{1}{n}\\sum_{i=1}^n(Y'_i-\\hat{Y}_i)^2\\big]=\\mathbb{E}\\big[\\frac{1}{n}\\sum_{i=1}^n(Y_i-\\hat{Y}_i)^2\\big]+\\frac{2(p+1)}{n}\\sigma^2$$\n",
    "\n",
    "\\end{theorem}\n",
    "\n",
    "\\begin{proof}\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}\\big[(Y_i-\\hat{Y}_i)^2\\big]&=\\mathbb{V}\\big[Y_i-\\hat{Y}_i\\big]+\\Big(\\mathbb{E}\\big[Y_i-\\hat{Y}_i\\big]\\Big)^2\\\\\n",
    "&=\\mathbb{V}[Y_i]+\\mathbb{V}\\big[\\hat{Y}_i\\big] - 2\\text{Cov}\\big[Y_i,\\hat{Y}_i\\big]+\\Big(\\mathbb{E}[Y_i]-\\mathbb{E}\\big[\\hat{Y}_i\\big]\\Big)^2\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}\\big[(Y'_i-\\hat{Y}_i)^2\\big]&=\\mathbb{V}\\big[Y'_i-\\hat{Y}_i\\big]+\\Big(\\mathbb{E}\\big[Y'_i-\\hat{Y}_i\\big]\\Big)^2\\\\\n",
    "&=\\mathbb{V}[Y'_i]+\\mathbb{V}\\big[\\hat{Y}_i\\big] - 2\\text{Cov}\\big[Y'_i,\\hat{Y}_i\\big]+\\Big(\\mathbb{E}[Y'_i]-\\mathbb{E}\\big[\\hat{Y}_i\\big]\\Big)^2\n",
    "\\end{align*}\n",
    "\n",
    "It is easy to see that $\\mathbb{E}[Y'_i]=\\mathbb{E}[Y_i]$, $\\mathbb{V}[Y'_i]=\\mathbb{V}[Y_i]$ and $\\text{Cov}\\big[Y'_i,\\hat{Y}_i\\big]=0$.\n",
    "\n",
    "Therefore\n",
    "\n",
    "$$\\mathbb{E}\\big[(Y'_i-\\hat{Y}_i)^2\\big]=\\mathbb{E}\\big[(Y_i-\\hat{Y}_i)^2\\big] + 2\\text{Cov}\\big[Y_i,\\hat{Y}_i\\big]$$\n",
    "\n",
    "Then take expectation\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}\\big[\\frac{1}{n}\\sum_{i=1}^n(Y'_i-\\hat{Y}_i)^2\\big]&=\\mathbb{E}\\big[\\frac{1}{n}\\sum_{i=1}^n(Y_i-\\hat{Y}_i)^2\\big]+\\frac{2}{n}\\sum_{i=1}^n\\text{Cov}\\big[Y_i,\\hat{Y}_i\\big]\\\\\n",
    "&=\\mathbb{E}\\big[\\frac{1}{n}\\sum_{i=1}^n(Y_i-\\hat{Y}_i)^2\\big]+\\frac{2}{n}\\sum_{i=1}^n\\sigma^2h_{ii}\\\\\n",
    "&=\\mathbb{E}\\big[\\frac{1}{n}\\sum_{i=1}^n(Y_i-\\hat{Y}_i)^2\\big]+\\frac{2}{n}\\sigma^2(p+1)\n",
    "\\end{align*}\n",
    "\\end{proof}\n",
    "\n",
    "The term $\\frac{2(p+1)}{n}\\sigma^2$ is called the **optimism** of the model. One can see it agrees on the learning theory\n",
    "\n",
    "1.More noise gives the model more opportunities to seem to fit well.\n",
    "\n",
    "2.More data makes it harder to pretend the fit is better than it really is.\n",
    "\n",
    "3.Growing $p$ makes it easy to fit noise, in other words, if adding a parameter does not reduce more a fixed amount of $MSE$, it should not be added.\n",
    "\n",
    "Therefore, the **Mallows $C_p$** statistic using debiased quantity\n",
    "\n",
    "$$C_p=\\frac{1}{n}\\sum_{i=1}^n(Y_i-\\hat{Y}_i)^2+\\frac{2\\hat{\\sigma}^2}{n}(p+1)=MSE+\\text{penalty} $$\n",
    "\n",
    "For model selection, we use\n",
    "\n",
    "$$\\Delta C_p= MSE_1-MSE_2+\\frac{2\\hat{\\sigma}^2}{n}(p_1-p_2)$$\n",
    "\n",
    "Let $\\hat{Y}_i^{(-i)}$ be the predicted value when we leave out point $(X_i,Y_i)$. Define the **leave-one-out-cross-validation score** as\n",
    "\n",
    "$$LOOCV\\equiv\\frac{1}{n}\\sum_{i=1}^n(Y_i-\\hat{Y}_i^{(-i)})^2=\\frac{1}{n}\\sum_{i=1}^n\\big(\\frac{Y_i-\\hat{Y}_i}{1-\\mathbf{H}_{ii}}\\big)^2$$\n",
    "\n",
    "The average of $\\mathbf{H}_{ii}$ is $\\gamma=\\frac{p+1}{n}$, here we use $\\gamma$ to approximate $\\mathbf{H}_{ii}$. Then\n",
    "\n",
    "\\begin{align*}\n",
    "LOOCV&\\approx \\frac{1}{n}\\sum_{i=1}^n\\big(\\frac{Y_i-\\hat{Y}_i}{1-\\gamma}\\big)^2\\\\\n",
    "&\\approx\\frac{1+2\\gamma}{n}\\sum_{i=1}^n(Y_i-\\hat{Y}_i)^2\\\\\n",
    "&=\\frac{1}{n}\\sum_{i=1}^n(Y_i-\\hat{Y}_i)^2+\\frac{2\\gamma}{n}\\sum_{i=1}^n(Y_i-\\hat{Y}_i)^2\\\\\n",
    "&=MSE+\\frac{2\\hat{\\sigma}^2}{n}(p+1)\n",
    "\\end{align*}\n",
    "\n",
    "Note that we use the Taylor series $(1+\\gamma)^{-2}\\approx 1+2\\gamma$. Therefore leave-one-out-cross-validation score is also a estimation of population error!\n",
    "\n",
    "$LOOCV$ or cross-validation estimator is the most popular tool to do model selection, for that it can estimate generalization error.\n",
    "\n",
    "Here we will introduce $AIC$, it estimates the error by subtract the number of parameters. Suppose the true parameter is $\\theta^*$, define the number of dimensions of parameter space $d\\equiv\\text{dim}(\\Theta)$. And the MLE of $\\theta$ is $\\hat{\\theta}$. Then\n",
    "\n",
    "$$\\nabla\\mathcal{L}(\\hat{\\theta})=0$$\n",
    "\n",
    "where $\\mathcal{L}(\\theta)$ is the log-likelihood under the sample. Do a Taylor series expansion of $\\nabla\\mathcal{L}(\\theta)$ around $\\theta^*$:\n",
    "\n",
    "$$\\nabla\\mathcal{L}(\\theta)=\\nabla\\mathcal{L}(\\theta^*)+(\\theta-\\theta^*)^T\\nabla\\nabla\\mathcal{L}(\\theta^*)$$\n",
    "\n",
    "Denote the Hessian matrix $\\mathbf{K}$\n",
    "\n",
    "$$\\mathbf{0}=\\nabla\\mathcal{L}(\\theta^*)+(\\theta-\\theta^*)\\mathbf{K}$$\n",
    "\n",
    "The expected log-likelihood of $\\hat{\\theta}$ is given by\n",
    "\n",
    "$$\\ell(\\theta)\\approx\\ell(\\theta^*)+(\\theta-\\theta^*)\\nabla\\ell(\\theta^*)+\\frac{1}{2}(\\theta-\\theta^*)^T\\nabla\\nabla\\ell(\\theta^*)(\\theta-\\theta^*)$$\n",
    "\n",
    "where $\\ell(\\theta)=\\mathbb{E}_{\\text{Data}}[\\mathcal{L}(\\theta)]$\n",
    "\n",
    "From parametric inference we know $\\mathbb{E}[\\nabla\\mathcal{L}(\\theta^*)]=0$,\n",
    "\n",
    "$$\\ell(\\theta)\\approx\\ell(\\theta^*)+\\frac{1}{2}(\\theta-\\theta^*)^T\\mathbf{k}(\\theta-\\theta^*)$$\n",
    "\n",
    "where $\\mathbf{k}$ is the Hessian matrix.\n",
    "\n",
    "Apply this to the MLE\n",
    "\n",
    "$$\\ell(\\hat{\\theta})\\approx\\ell(\\theta^*)+\\frac{1}{2}\\nabla\\mathcal{L}(\\theta^*)\\mathbf{K}^{-1}\\mathbf{k}\\mathbf{K}^{-1}\\nabla\\mathcal{L}(\\theta^*)$$\n",
    "\n",
    "Taking expectations,\n",
    "\n",
    "$$\\mathbb{E}\\big[\\ell(\\hat{\\theta})\\big]\\approx\\ell(\\theta^*)+\\frac{1}{2}\\text{tr}(\\mathbf{K}^{-1}\\mathbf{k}\\mathbf{K}^{-1}\\mathbf{J})$$\n",
    "\n",
    "where $\\mathbf{J}$ is the **Fisher Information matrix**, i.e the variance of score function. From the Law of Large Numbers, $\\forall\\theta,\\mathbb{E}[\\mathcal{L}(\\theta)]\\stackrel{P}\\rightarrow\\ell(\\theta)$. Adding the continuous mapping theorem, we have $\\nabla\\nabla\\mathcal{L}(\\theta^*)=\\mathbf{K}\\stackrel{P}\\rightarrow\\mathbf{k}=\\nabla\\nabla\\ell(\\theta^*)$.\n",
    "\n",
    "$$\\mathbb{E}\\big[\\ell(\\hat{\\theta})\\big]\\approx\\ell(\\theta^*)+\\frac{1}{2}\\text{tr}(\\mathbf{K}^{-1}\\mathbf{J})$$\n",
    "\n",
    "How to deal with the unknown $\\ell(\\theta^*)$? Another Taylor expansion\n",
    "\n",
    "$$\\mathcal{L}(\\theta^*)\\approx\\mathcal{L}(\\hat{\\theta}) +\\frac{1}{2}(\\theta^*-\\hat{\\theta})^T\\nabla\\nabla\\mathcal{L}(\\hat{\\theta})(\\theta^*-\\hat{\\theta})$$\n",
    "\n",
    "Plug into the MLE\n",
    "\n",
    "$$\\mathcal{L}(\\theta^*)\\approx\\mathcal{L}(\\hat{\\theta}) +\\frac{1}{2}(\\mathbf{K}^{-1}\\nabla\\mathcal{L}(\\theta^*))^T\\nabla\\nabla\\mathcal{L}(\\hat{\\theta})(\\mathbf{K}^{-1}\\nabla\\mathcal{L}(\\theta^*))$$\n",
    "\n",
    "For $\\hat{\\theta}\\stackrel{P}\\rightarrow\\theta^*$, according to continuous mapping theorem we have $$\\nabla\\nabla\\mathcal{L}(\\hat{\\theta})\\stackrel{P}\\rightarrow\\nabla\\nabla\\mathcal{L}(\\theta^*)$$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\\mathcal{L}(\\theta^*)\\approx\\mathcal{L}(\\hat{\\theta}) +\\frac{1}{2}\\text{tr}(\\mathbf{k}^{-1}\\mathbf{J}) $$\n",
    "\n",
    "Take expectations,\n",
    "\n",
    "$$\\ell(\\theta^*)=\\mathbb{E}[\\mathcal{L}(\\hat{\\theta})]+\\frac{1}{2}\\text{tr}(\\mathbf{k}^{-1}\\mathbf{J})$$\n",
    "\n",
    "Therefore\n",
    "\n",
    "$$\\ell(\\hat{\\theta})\\approx\\ell(\\theta^*)+\\frac{1}{2}\\text{tr}(\\mathbf{k}^{-1}\\mathbf{J})=\\mathbb{E}[\\mathcal{L}(\\hat{\\theta})]+\\text{tr}(\\mathbf{k}^{-1}\\mathbf{J})$$\n",
    "\n",
    "And it is known that under regularity condition, we have\n",
    "\n",
    "$$\\mathbf{J}=-\\mathbb{E}[\\mathbf{K}]=-\\mathbf{k}$$\n",
    "\n",
    "Then \n",
    "\n",
    "$$\\ell(\\hat{\\theta})\\approx\\mathbb{E}[\\mathcal{L}(\\hat{\\theta})]-\\text{tr}[\\mathbf{I}_{d\\times d}] =\\mathbb{E}[\\mathcal{L}(\\hat{\\theta})]-d$$\n",
    "\n",
    "The unbiased estimator is\n",
    "\n",
    "$$AIC=\\mathcal{L}(\\hat{\\theta})-d$$\n",
    "\n",
    "In order to select linear Gaussian model, it is easy to show that\n",
    "\n",
    "$$\\frac{2\\hat{\\sigma}^2}{n}\\Delta AIC \\approx \\Delta MSE+\\frac{2}{n}\\hat{\\sigma}^2(p_1-p_2)=C_p$$\n",
    "\n",
    "It is also the approximation of the population error.\n",
    "\n",
    "The last criterion is $BIC$,\n",
    "\n",
    "$$BIC=\\mathcal{L}(\\hat{\\theta})-\\frac{\\log{n}}{2}d$$\n",
    "\n",
    "In fact, $BIC$ is trying to maximize $\\mathbb{P}(\\mathcal{M}_j\\mid\\text{Data})\\propto p(\\text{Data}\\mid\\mathcal{M}_j)p_j$.\n",
    "\n",
    "It is shown that $BIC$ tends to choose simpler model. Note that $BIC$ assumes that one of the models is true and we are trying to find the model most likely to be true in the Bayesian scenario. Instead, $AIC$ and $LOOCV$ try to find the model predicting best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heteroskedasticity and Weighted Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reality, we may encounter the prediction accuracy requirement where the point is influenced more by its neighbors or the heteroskedasticity.\n",
    "\n",
    "Here we consider the linear model\n",
    "\n",
    "1.$\\mathbf{Y}=\\mathbf{X}\\beta+\\epsilon$\n",
    "\n",
    "2.$\\mathbb{E}[\\epsilon_i\\mid x_i]=0$ and $\\mathbb{V}[\\epsilon_i\\mid x_i]=\\sigma^2_i$, or $\\mathbb{V}[\\epsilon\\mid \\mathbf{X}]=\\Sigma$.\n",
    "\n",
    "3.$\\epsilon$ are independent across observations.\n",
    "\n",
    "We use the **Weighted Least Squares estimator** $K$ which minimize the weighted $MSE$ to estimate $\\beta$.\n",
    "\n",
    "$$WMSE=\\frac{1}{n}\\sum_{i=1}w_i(Y_i-\\mathbf{X}_i\\beta)^2$$\n",
    "\n",
    "Where $K=(\\mathbf{X}^T\\mathbf{W}\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{W}$. Then\n",
    "\n",
    "$$\\mathbb{E}[\\hat{\\beta}]=\\beta$$\n",
    "\n",
    "$$\\mathbb{V}[\\hat{\\beta}]=K\\Sigma K^T=(\\mathbf{X}^T\\Sigma^{-1}\\mathbf{X})^{-1}$$.\n",
    "\n",
    "Therefore, $\\hat{\\beta}=K\\mathbf{Y}$ is the unbiased estimator of $\\beta$.\n",
    "\n",
    "\\begin{theorem}[Gauss-Markov]\n",
    "\n",
    "$\\hat{\\beta}=(\\mathbf{X}^T\\mathbf{W}\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{W}\\mathbf{Y}$ is the best linear unbiased estimator of $\\beta$.\n",
    "\n",
    "In other words, for all linear unbiased estimators $\\tilde{\\beta}$.\n",
    "$$\\mathbb{V}[\\tilde{\\beta}]\\geq \\mathbb{V}[\\hat{\\beta}]$$\n",
    "\n",
    "\\end{theorem}\n",
    "\n",
    "\n",
    "\\begin{proof}\n",
    "\n",
    "Let $\\tilde{\\beta}=\\mathbf{QY}$ be any linear unbiased estimator. Then\n",
    "\n",
    "$$\\mathbb{E}[\\mathbf{QY}]=\\mathbf{QX}\\beta$$\n",
    "\n",
    "$\\mathbf{QX}$ has to be $\\mathbf{I}$.\n",
    "\n",
    "Then $\\mathbb{V}[\\mathbf{QY}\\mid \\mathbf{X}]=\\mathbf{Q}\\Sigma\\mathbf{Q}^T$.\n",
    "\n",
    "Let $K\\equiv (\\mathbf{X}^T\\Sigma^{-1}\\mathbf{X})^{-1}\\mathbf{X}^T\\Sigma^{-1}$. Whatever $\\mathbf{Q}$ might be, we can find a matrix $\\mathbf{R}$ that\n",
    "\n",
    "$$\\mathbf{Q}=K+\\mathbf{R}$$\n",
    "\n",
    "And from $K\\mathbf{X}=\\mathbf{I}$, we have $\\mathbf{RX}=\\mathbf{0}$.\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{V}[\\tilde{\\beta}]&=(K+\\mathbf{R})\\Sigma(K+\\mathbf{R})^T\\\\\n",
    "&=K\\Sigma K+\\mathbf{R}\\Sigma K^T +K\\Sigma\\mathbf{R}^T+ \\mathbf{R}\\Sigma\\mathbf{R}^T\\\\\n",
    "&=(\\mathbf{X}^T\\Sigma^{-1}\\mathbf{X})^{-1}+\\mathbf{R}\\Sigma\\mathbf{R}^T\\\\\n",
    "&=\\mathbb{V}[\\hat{\\beta}]+\\mathbf{R}\\Sigma\\mathbf{R}^T\n",
    "\\end{align*}\n",
    "\n",
    "The last term is positive definate, hence\n",
    "\n",
    "$$\\mathbb{V}[\\tilde{\\beta}]\\geq \\mathbb{V}[\\hat{\\beta}]$$\n",
    "\n",
    "\\end{proof}\n",
    "\n",
    "Note that\n",
    "\n",
    "1.If all the noise variances are equal, we obtain the optimality of OLS.\n",
    "\n",
    "2.The theorem does not rule out the linear biased estimators with smaller variance or non-linear biased estimators of even smaller variance.\n",
    "\n",
    "If we do not know the variances, there are two common ways to estimate conditional variances.\n",
    "\n",
    "Method 1:\n",
    "\n",
    "1.Estimate $m(x)$ with a regression model $\\hat{m}(x)$.\n",
    "\n",
    "2.Construct the squared residuals $u_i=(Y_i-\\hat{m}(x_i))^2$.\n",
    "\n",
    "3.Use any non-parametric method to estimate the conditional mean of $u_i$, denote $\\hat{q}(x)$.\n",
    "\n",
    "4.Estimate the variance using $\\hat{\\sigma}^2_x=\\hat{q}(x)$.\n",
    "\n",
    "Method 2:\n",
    "\n",
    "1.Estimate $m(x)$ with a regression model $\\hat{m}(x)$.\n",
    "\n",
    "2.Construct log squared residuals $z_i=\\log{(Y_i-\\hat{m}(x_i))^2}$.\n",
    "\n",
    "3.Use any non-parametric method to estimate the conditional mean of $z_i$, denote $\\hat{s}(x)$.\n",
    "\n",
    "4.Predict the variance using $\\hat{\\sigma}^2_x=e^{\\hat{s}(x)}$\n",
    "\n",
    "Method 2 ensures the estimated variances are positive!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>3.96174962094616</li>\n",
       "\t<li>6.99288089457596</li>\n",
       "\t<li>7.53420579103561</li>\n",
       "\t<li>9.44961873668519</li>\n",
       "\t<li>12.0119764025398</li>\n",
       "\t<li>14.7744432357496</li>\n",
       "\t<li>15.2408546646743</li>\n",
       "\t<li>18.0623647137398</li>\n",
       "\t<li>18.1889888000663</li>\n",
       "\t<li>20.7458488039048</li>\n",
       "\t<li>26.472592992342</li>\n",
       "\t<li>22.3660474307336</li>\n",
       "\t<li>20.2124017744625</li>\n",
       "\t<li>33.0288020091828</li>\n",
       "\t<li>28.1262614537505</li>\n",
       "\t<li>36.4361687030798</li>\n",
       "\t<li>33.238801930916</li>\n",
       "\t<li>33.4390912625442</li>\n",
       "\t<li>41.3149865719803</li>\n",
       "\t<li>40.6509273681527</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 3.96174962094616\n",
       "\\item 6.99288089457596\n",
       "\\item 7.53420579103561\n",
       "\\item 9.44961873668519\n",
       "\\item 12.0119764025398\n",
       "\\item 14.7744432357496\n",
       "\\item 15.2408546646743\n",
       "\\item 18.0623647137398\n",
       "\\item 18.1889888000663\n",
       "\\item 20.7458488039048\n",
       "\\item 26.472592992342\n",
       "\\item 22.3660474307336\n",
       "\\item 20.2124017744625\n",
       "\\item 33.0288020091828\n",
       "\\item 28.1262614537505\n",
       "\\item 36.4361687030798\n",
       "\\item 33.238801930916\n",
       "\\item 33.4390912625442\n",
       "\\item 41.3149865719803\n",
       "\\item 40.6509273681527\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 3.96174962094616\n",
       "2. 6.99288089457596\n",
       "3. 7.53420579103561\n",
       "4. 9.44961873668519\n",
       "5. 12.0119764025398\n",
       "6. 14.7744432357496\n",
       "7. 15.2408546646743\n",
       "8. 18.0623647137398\n",
       "9. 18.1889888000663\n",
       "10. 20.7458488039048\n",
       "11. 26.472592992342\n",
       "12. 22.3660474307336\n",
       "13. 20.2124017744625\n",
       "14. 33.0288020091828\n",
       "15. 28.1262614537505\n",
       "16. 36.4361687030798\n",
       "17. 33.238801930916\n",
       "18. 33.4390912625442\n",
       "19. 41.3149865719803\n",
       "20. 40.6509273681527\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1]  3.961750  6.992881  7.534206  9.449619 12.011976 14.774443 15.240855\n",
       " [8] 18.062365 18.188989 20.745849 26.472593 22.366047 20.212402 33.028802\n",
       "[15] 28.126261 36.436169 33.238802 33.439091 41.314987 40.650927"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x <- c(1:20)\n",
    "eps <- c(1:20) \n",
    "eps[1:10] <- rnorm(mean = 0, sd = 1, n = 10)\n",
    "eps[11:20] <- rnorm(mean = 0, sd = 4, n = 10)\n",
    "beta_0 = 1.0\n",
    "beta_1 = 2.0\n",
    "y <- beta_0 + beta_1 * x + eps\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAM1BMVEUAAABNTU1oaGh8fHyM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",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "par(fin = c(3,3), pin = c(3 ,3))\n",
    "plot(x, y, pch = 19, col = \"gray\")\n",
    "out <- lm(y ~ x)\n",
    "abline(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = y ~ x)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-6.6383 -1.1087 -0.1916  1.0406  4.2828 \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)   2.2117     1.2317   1.796   0.0894 .  \n",
       "x             1.8953     0.1028  18.433 3.92e-13 ***\n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 2.652 on 18 degrees of freedom\n",
       "Multiple R-squared:  0.9497,\tAdjusted R-squared:  0.9469 \n",
       "F-statistic: 339.8 on 1 and 18 DF,  p-value: 3.921e-13\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAM1BMVEUAAABNTU1oaGh8fHyM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\nop29GA61AAAAAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(out)\n",
    "par(fin = c(3,3), pin = c(3 ,3))\n",
    "plot(x, rstudent(out), pch = 19, col = \"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAM1BMVEUAAABNTU1oaGh8fHyM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",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "u = log((resid(out))^2)\n",
    "tmp = loess(u ~ x)\n",
    "s_2 = exp(tmp$fitted)\n",
    "par(fin = c(3,3), pin = c(3 ,3))\n",
    "plot(x, s_2, pch = 19, col = \"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = y ~ x, weights = w)\n",
       "\n",
       "Weighted Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-1.9403 -1.1581 -0.6041  0.9872  2.5370 \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)  2.29940    0.29815   7.712 4.11e-07 ***\n",
       "x            1.91926    0.04868  39.425  < 2e-16 ***\n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 1.438 on 18 degrees of freedom\n",
       "Multiple R-squared:  0.9886,\tAdjusted R-squared:  0.9879 \n",
       "F-statistic:  1554 on 1 and 18 DF,  p-value: < 2.2e-16\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w = 1/s_2\n",
    "out_2 = lm(y ~ x, weights = w)\n",
    "summary(out_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\hat{m}(x)$ denote the estimated model and the true regression function is $m(x)$, if $\\hat{m}(x)$ is the unbiased estimator of $m(x)$. However, generally it is not true because the model assumptions are wrong. And even if the true model is linear, we could also have bias because of the variable selection.\n",
    "\n",
    "Define the prediction error of a testing observation\n",
    "\n",
    "$$R\\equiv \\mathbb{E}(Y-m(X))^2=\\mathbb{E}\\big(Y-m(X)+m(X)-\\mathbb{E}[\\hat{m}(X)]+\\mathbb{E}[\\hat{m}(X)]-\\hat{m}(X)\\big)^2=\\tau^2+B^2+V$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\\tau^2=\\mathbb{E}(Y-m(X))^2$$\n",
    "\n",
    "$$B^2=\\mathbb{E}\\big[(\\mathbb{E}[\\hat{m}(X)]-m(X))^2\\big]$$\n",
    "\n",
    "$$V=\\mathbb{E}\\big[(\\hat{m}(X)-\\mathbb{E}[\\hat{m}(X)])^2\\big]$$\n",
    "\n",
    "We call $\\tau^2$ is the unavoidable error, it means even if we know the true model $m(X)$, we still make mistakes. And $B^2$ is the squared bias term, $V$, the variance.\n",
    "\n",
    "From learning theory, small models have low variance and high bias. Large models have high variance and low bias.\n",
    "\n",
    "A trivial thought to picking a variable under the assumption is using the $p$-value. But it is not a good method. Recall the $t$-test of coefficients\n",
    "\n",
    "$$t=\\frac{\\hat{\\beta_j}}{\\hat{\\text{se}}[\\hat{\\beta_j}]}=\\frac{\\hat{\\beta_j}}{\\frac{\\hat{\\sigma}}{\\sqrt{ns_{X_i}}}\\sqrt{VIF_i}}$$\n",
    "\n",
    "Note that the defination of $VIF$ is the ratio between correlated predictors variance and uncorrelated predictors variance. Therefore\n",
    "\n",
    "1.Larger coefficients will have larger test statistics and be more significant.\n",
    "\n",
    "2.Reducing the noise around the regression line will increase **all** the test statistics.\n",
    "\n",
    "3.Increasing the sample size will increase **all** the test statistics.\n",
    "\n",
    "4.More variance in a predictor variable will increase the test statistics with others unchanged.\n",
    "\n",
    "5.More correlation between $X_i$ and others will decrease the test statistics of $i$.\n",
    "\n",
    "Therefore how to fit all the models? Here is the solution.\n",
    "\n",
    "The preprocessing method is scaling the designed matrix and response.\n",
    "\n",
    "And the **Forward Stepwise Regresion** is follwing\n",
    "\n",
    "1.Start by fitting the simplest model $Y=\\beta_0+\\epsilon$. Let $S=\\emptyset$.\n",
    "\n",
    "2.Next consider all single variable models. Select the one with lowest $AIC$ or cross-validation error.\n",
    "\n",
    "3.Adding another variable. Choose the best one adding to $S$.\n",
    "\n",
    "4.Continue until can not add.\n",
    "\n",
    "So we have model $\\mathcal{M}_1,\\mathcal{M}_2,\\dots$, we need to choose the one with lowest prediction error.\n",
    "\n",
    "Another approach is to use **lasso**, which minimize\n",
    "\n",
    "$$(\\mathbf{Y}-\\mathbf{X}\\beta)^T(\\mathbf{Y}-\\mathbf{X}\\beta)+\\lambda\\|\\beta\\|_1$$\n",
    "\n",
    "It turns out that the solution is sparse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'Air.Flow'</li>\n",
       "\t<li>'Water.Temp'</li>\n",
       "\t<li>'Acid.Conc.'</li>\n",
       "\t<li>'stack.loss'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'Air.Flow'\n",
       "\\item 'Water.Temp'\n",
       "\\item 'Acid.Conc.'\n",
       "\\item 'stack.loss'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'Air.Flow'\n",
       "2. 'Water.Temp'\n",
       "3. 'Acid.Conc.'\n",
       "4. 'stack.loss'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"Air.Flow\"   \"Water.Temp\" \"Acid.Conc.\" \"stack.loss\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data(stackloss)\n",
    "names(stackloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start:  AIC=98.4\n",
      "stack.loss ~ 1\n",
      "\n",
      "             Df Sum of Sq     RSS    AIC\n",
      "+ Air.Flow    1    1750.1  319.12 61.142\n",
      "+ Water.Temp  1    1586.1  483.15 69.852\n",
      "+ Acid.Conc.  1     330.8 1738.44 96.741\n",
      "<none>                    2069.24 98.399\n",
      "\n",
      "Step:  AIC=61.14\n",
      "stack.loss ~ Air.Flow\n",
      "\n",
      "             Df Sum of Sq    RSS    AIC\n",
      "+ Water.Temp  1   130.321 188.80 52.119\n",
      "<none>                    319.12 61.142\n",
      "+ Acid.Conc.  1     9.979 309.14 62.475\n",
      "\n",
      "Step:  AIC=52.12\n",
      "stack.loss ~ Air.Flow + Water.Temp\n",
      "\n",
      "             Df Sum of Sq    RSS    AIC\n",
      "<none>                    188.79 52.119\n",
      "+ Acid.Conc.  1    9.9654 178.83 52.980\n"
     ]
    }
   ],
   "source": [
    "small = lm(stack.loss ~ 1, data = stackloss)\n",
    "big = lm(stack.loss ~ ., data = stackloss)\n",
    "tmp = step(small, scope = list(lower = small, upper = big), direction = \"forward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'lars' was built under R version 3.4.1\"Loaded lars 1.2\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 3\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAM1BMVEUAAABNTU1oaGh8fHyM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\nAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(lars)\n",
    "x <- as.matrix(stackloss[, 1:3])\n",
    "y <- stack.loss\n",
    "out = lars(x, y, type = \"stepwise\")\n",
    "tmp = cv.lars(x, y, K = 10, type = \"stepwise\", plot.it = FALSE)\n",
    "m = length(tmp$cv)\n",
    "par(fin = c(3,3), pin = c(3 ,3))\n",
    "plot(1:m, tmp$cv, pch = 19, col = \"gray\")\n",
    "for (i in 1:m){\n",
    "    segments(i, tmp$cv[i] - tmp$cv.error[i], i, tmp$cv[i] + tmp$cv.error[i])\n",
    "}\n",
    "j = which.min(tmp$cv)\n",
    "print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'glmnet' was built under R version 3.4.2\"Loading required package: Matrix\n",
      "Loading required package: foreach\n",
      "Warning message:\n",
      "\"package 'foreach' was built under R version 3.4.2\"Loaded glmnet 2.0-13\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAANlBMVEUAAAAAzQBNTU1oaGh8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\nSIAARQIEKBIgQJEAAYoECFAkQOD/Y+4IBqTVxTkAAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(glmnet)\n",
    "out = glmnet(x, y)\n",
    "par(fin = c(3,3), pin = c(3 ,3))\n",
    "plot(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"Option grouped=FALSE enforced in cv.glmnet, since < 3 observations per fold\""
     ]
    },
    {
     "data": {
      "text/html": [
       "0.0454353847844836"
      ],
      "text/latex": [
       "0.0454353847844836"
      ],
      "text/markdown": [
       "0.0454353847844836"
      ],
      "text/plain": [
       "[1] 0.04543538"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAANlBMVEUAAABNTU1oaGh8fHyM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",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cvfit = cv.glmnet(x, y)\n",
    "par(fin = c(3,3), pin = c(3 ,3))\n",
    "plot(cvfit)\n",
    "cvfit$lambda.min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4 x 1 sparse Matrix of class \"dgCMatrix\"\n",
       "                      1\n",
       "(Intercept) -40.7641798\n",
       "Air.Flow      0.7078085\n",
       "Water.Temp    1.2867835\n",
       "Acid.Conc.   -0.1347717"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "coef(cvfit, s = \"lambda.min\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "31px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
